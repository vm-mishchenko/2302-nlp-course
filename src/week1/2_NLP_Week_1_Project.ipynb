{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFuV20LKc0h3"
   },
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "# Week 1: Text Classification\n",
    "\n",
    "### What are we building\n",
    "We’ll continue to apply our learning philosophy of repetition as we build multiple classification models of increasing complexity in the following order:\n",
    "\n",
    "1. Average of Word2Vec + MLP Layer\n",
    "1. Can we concatenate 3 token embeddings and then average them? Does this do better than the previous method?\n",
    "1. Build an embedding layer based model.\n",
    "1. **Extension**: Explore different parameters, features and architectures. \n",
    "\n",
    "###  Evaluation\n",
    "We’ll be evaluating our models on the following metric: \n",
    "\n",
    "1. Accuracy: is the ratio of the number of correctly classified instances to the total number of instances\n",
    "1. **Extension**: this is a multi-class classification problem, visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#confusion-matrix-func) of N*N of actual class vs predicted class (N = number of classes).\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We've provide scaffolding for all the boiler plate PyTorch code to get to our first model. This covers downloading and parsing the dataset, training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point our model gets to an accuracy of about 0.32. After this we'll try to improve the model by using sliding windows of text instead of just one word at a time. **Does this improve accuracy?**\n",
    "1. The third model we're going to build is an embedding layer based model. Here instead of using pre-trained word-embeddings we'll be creating new vectors as part of the training process. **How do you think this model will perform?**\n",
    "1. **Extension**: We've suggested a bunch of extensions to the project so go crazy, tweak any parts of the pipeline and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "- Dependencies: Python dependencies and loading the spacy model\n",
    "- Project\n",
    "  - Dataset: Download the conversation dataset and parse it into a pytorch Dataset\n",
    "  - Trainer: Trainer function to help with multi-epoch training\n",
    "  - Model 1: Simple Word2Vec + MLP model\n",
    "  - Model 2: Sliding window trigram (Word2Vec)\n",
    "  - Model 3: Embedding bag based model on Trigram\n",
    "- Extensions\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TWogdWh1MWp"
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "✨ Now let's get started and to kick things off as always we install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uj-w6eV7_abU"
   },
   "source": [
    "# Install all the required dependencies for the project\n",
    "!pip install pytorch-lightning==1.6.5 spacy==2.2.4\n",
    "!python -m spacy download en_core_web_md\n",
    "!pip install pandas"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGpYPUn2uVBK"
   },
   "source": [
    "Import all the necessary libraries we need throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NSDbwEoL8-0r"
   },
   "source": [
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from collections import Counter\n",
    "import en_core_web_md\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import pandas as pd"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyS2zGroubkB"
   },
   "source": [
    "First things first, let's load the Spacy data which comes with pre-trainined embeddings. This process is expensive so only do this once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z6EcIDXdcd-E"
   },
   "source": [
    "# Really expensive operation to load the entire space word-vector index in memory\n",
    "# We'll only run it once \n",
    "loaded_spacy_model = en_core_web_md.load()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riBtB6GDur-W"
   },
   "source": [
    "Fix the random seed for numpy and pytorch so the entire class gets consistent results which we can discuss with each other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7pOF_7XXQ6M"
   },
   "source": [
    "# Fix the random seed so that we get consistent results\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier Project \n",
    "\n",
    "✨ Let's Begin ✨"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0O4OYJzPxT-"
   },
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We’ll be using the Empathetic Dialogs dataset open-sourced by Facebook ([link](https://research.fb.com/publications/towards-empathetic-open-domain-conversation-models-a-new-benchmark-and-dataset/)). It can be downloaded as a tar ball from the following [link](https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz)\n",
    "\n",
    "A sample row from the dataset: \n",
    "```\n",
    "conv_id,utterance_idx,context,prompt,speaker_idx,utterance,selfeval,tags\n",
    "hit:12388_conv:24777,1,joyful,I felt overcome with emotions when Christmas came around as a kid,437,Christmas was the best time of year back in the day!,5|5|5_5|5|5, ''\n",
    "```\n",
    "\n",
    "The three columns we'll primarily focus on are:\n",
    "1. \"context\" column ==> it's an emotion we're trying to predict\n",
    "1. \"prompt\" + \"utterance\" columns ==> We'll combine these sentences and use them as input\n",
    "\n",
    "But let's download and explore the dataset and these should automatically get clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_rMhXBWFrt4u"
   },
   "source": [
    "import tarfile\n",
    "import os\n",
    "import csv\n",
    "\n",
    "DIRECTORY_NAME=\"classification\"\n",
    "TRAIN_FILE=\"classification/empatheticdialogues/train.csv\"\n",
    "VALIDATION_FILE=\"classification/empatheticdialogues/valid.csv\"\n",
    "TEST_FILE=\"classification/empatheticdialogues/test.csv\"\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "  \"\"\"\n",
    "  Download the dialog dataset. The tarball contains three files: train.csv, valid.csv, test.csv \n",
    "  \"\"\"\n",
    "\n",
    "  # if running locally, install wget before by running: \"brew install wget\"\n",
    "  # \"!pip install wget\" won't help as it installs python module not cli\n",
    "  !wget 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "  # !wget help\n",
    "  if not os.path.isdir(DIRECTORY_NAME):\n",
    "    !mkdir classification\n",
    "  tar = tarfile.open('empatheticdialogues.tar.gz')\n",
    "  tar.extractall(DIRECTORY_NAME)\n",
    "  tar.close()\n",
    "\n",
    "# Expensive operation so we should just do this once\n",
    "download_dataset()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcQP1uLf6m-O"
   },
   "source": [
    "Now the question is that did it do the right thing? Time to find out.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xi753ebz6qWE"
   },
   "source": [
    "import glob\n",
    "glob.glob(f\"{DIRECTORY_NAME}/**/*.csv\", recursive=True)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['classification/empatheticdialogues/valid.csv',\n 'classification/empatheticdialogues/test.csv',\n 'classification/empatheticdialogues/train.csv']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlXMxZ51Puqz"
   },
   "source": [
    "Cool we see all our files. Let's poke at one of them before we start parsing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# See the parse_dataset function below for short explanation.\n",
    "df = pd.read_csv(TRAIN_FILE, sep='\\n', header=None)\n",
    "df = df[0].str.split(',', expand=True)\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.columns = new_header\n",
    "df.head(5)"
   ],
   "metadata": {
    "id": "kw_H0DHA0Yq2"
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0       conv_id utterance_idx      context  \\\n1  hit:0_conv:1             1  sentimental   \n2  hit:0_conv:1             2  sentimental   \n3  hit:0_conv:1             3  sentimental   \n4  hit:0_conv:1             4  sentimental   \n5  hit:0_conv:1             5  sentimental   \n\n0                                             prompt speaker_idx  \\\n1  I remember going to the fireworks with my best...           1   \n2  I remember going to the fireworks with my best...           0   \n3  I remember going to the fireworks with my best...           1   \n4  I remember going to the fireworks with my best...           0   \n5  I remember going to the fireworks with my best...           1   \n\n0                                          utterance     selfeval tags  \n1  I remember going to see the fireworks with my ...  5|5|5_2|2|5       \n2  Was this a friend you were in love with_comma_...  5|5|5_2|2|5       \n3                This was a best friend. I miss her.  5|5|5_2|2|5       \n4                                Where has she gone?  5|5|5_2|2|5       \n5                                 We no longer talk.  5|5|5_2|2|5       ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conv_id</th>\n      <th>utterance_idx</th>\n      <th>context</th>\n      <th>prompt</th>\n      <th>speaker_idx</th>\n      <th>utterance</th>\n      <th>selfeval</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>hit:0_conv:1</td>\n      <td>1</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>1</td>\n      <td>I remember going to see the fireworks with my ...</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hit:0_conv:1</td>\n      <td>2</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>0</td>\n      <td>Was this a friend you were in love with_comma_...</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hit:0_conv:1</td>\n      <td>3</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>1</td>\n      <td>This was a best friend. I miss her.</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hit:0_conv:1</td>\n      <td>4</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>0</td>\n      <td>Where has she gone?</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>hit:0_conv:1</td>\n      <td>5</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>1</td>\n      <td>We no longer talk.</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_NeRAwXQa26"
   },
   "source": [
    "The set of columns we care about are:\n",
    "1. context ==> emotion we're trying to predict\n",
    "1. prompt + utterance ==> We'll combine these sentences and use them as input \n",
    "\n",
    "Let's create a label encoder which converts our text labels to integer ids or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \"sentimental\" to \"<id>\"\n",
    "label_to_integer = dict()\n",
    "# \"<id>\" to \"sentimental\"\n",
    "integer_to_label = dict()\n",
    "\n",
    "for ix, label in enumerate(df[\"context\"].unique()):\n",
    "  label_to_integer[label] = ix\n",
    "  integer_to_label[ix] = label"
   ],
   "metadata": {
    "id": "iok05RN0CZd-"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhqgroR6P0uM"
   },
   "source": [
    "Parse the dataset file and create a label encoder that converts text labels to integer ids or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Krz1X_9PwRG"
   },
   "source": [
    "def parse_dataset(file_path, sample=5000):\n",
    "  # Our dataset file is a csv with varying input lengths, therefore we load the \n",
    "  # file at once, we have to split it up into separate steps:\n",
    "  # 1. Read each row as a single column row\n",
    "  df = pd.read_csv(file_path, sep = '\\n', header = None)\n",
    "  # 2. Split up each row into separate columns\n",
    "  df = df[0].str.split(',', expand = True)\n",
    "  # 3. Set the header by using the first row\n",
    "  new_header = df.iloc[0]\n",
    "  df = df[1:]\n",
    "  df.columns = new_header\n",
    "\n",
    "  # Machine learning cannot work with categorical labels like \"surprised\" or\n",
    "  # \"excited\". Therefore, we convert these tokens into a number.\n",
    "  df[\"target\"] = df[\"context\"].apply(lambda x: label_to_integer[x])\n",
    "  df[\"feature\"] = df[\"prompt\"] + \" \" + df[\"utterance\"]\n",
    "\n",
    "  # We only need the column \"**feature**\" created from column \n",
    "  # \"**prompt**\" + \"**utterance**\" and the column \"**target**\".\n",
    "  return df[[\"target\", \"feature\"]].sample(n = sample, random_state = 0).values"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the global training, validation and test datasets from the data files. We've limited our sample size to speed up the training of the models. Originally these are the sizes of the dataset: \n",
    "\n",
    "![Screen Shot 2022-07-20 at 8.56.08 AM.png](data:image/png;base64,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)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-ALx9Q1XP5rf"
   },
   "source": [
    "training_data = parse_dataset(TRAIN_FILE, sample = 40000)\n",
    "validation_data = parse_dataset(VALIDATION_FILE, sample = 4000)\n",
    "test_data = parse_dataset(TEST_FILE, sample = 4000)\n",
    "\n",
    "print('Shape of training dataset: ({rows}, {cols})'.format(rows=len(training_data), cols=len(training_data[0])))\n",
    "print('Shape of validation dataset: ({rows}, {cols})'.format(rows=len(validation_data), cols=len(validation_data[0])))\n",
    "print('Shape of test dataset: ({rows}, {cols})'.format(rows=len(test_data), cols=len(test_data[0])))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: (40000, 2)\n",
      "Shape of validation dataset: (4000, 2)\n",
      "Shape of test dataset: (4000, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-Uqb3NpWNoD"
   },
   "source": [
    "[Dataset and Data loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html): Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "[LightingDataModule](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html#datamodules): A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "\n",
    "1. Download / tokenize / process.\n",
    "2. Clean and (maybe) save to disk.\n",
    "3. Load inside Dataset.\n",
    "4. Apply transforms (rotate, tokenize, etc…).\n",
    "5. Wrap inside a DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H9cVcHmMg6Lj"
   },
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "  \"\"\"Creates an pytorch dataset to consume our pre-loaded csv data\n",
    "\n",
    "  Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html \n",
    "  \"\"\"\n",
    "  def __init__(self, data, vectorizer):\n",
    "    self.dataset = data\n",
    "    # Vectorizer needs to implement a vectorize function that returns vector and tokens\n",
    "    # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "    self.vectorizer = vectorizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    (label, sentence) = self.dataset[idx]\n",
    "    sentence_vector, sentence_tokens = self.vectorizer.vectorize(sentence)\n",
    "    return {\n",
    "        \"vectors\": sentence_vector,\n",
    "        \"label\": label,\n",
    "        \"tokens\": sentence_tokens, # for debugging only\n",
    "        \"sentence\": sentence # for debugging only\n",
    "      }\n",
    "\n",
    "class ClassificationDataModule(pl.LightningDataModule):\n",
    "  \"\"\"LightningDataModule: Wrapper class for the dataset to be used in training\n",
    "  \"\"\"\n",
    "  def __init__(self, vectorizer, params):\n",
    "    super().__init__()\n",
    "    self.params = params\n",
    "    self.classification_train = ClassificationDataset(training_data, vectorizer)\n",
    "    self.classification_val = ClassificationDataset(validation_data, vectorizer)\n",
    "    self.classification_test = ClassificationDataset(test_data, vectorizer)\n",
    "\n",
    "  # Function to convert the input raw data from the dataset into model input. \n",
    "  # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "  def collate_fn(self, batch):\n",
    "    # Embedding layers need the inputs to be integer, so we need to add this special case here.\n",
    "    if self.params.integer_input: \n",
    "      word_vector = [torch.LongTensor(item[\"vectors\"]) for item in batch]\n",
    "      sentence_vector = pad_sequence(word_vector, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "      sentence_vector = torch.stack([torch.Tensor(item[\"vectors\"]) for item in batch])\n",
    "    labels = torch.LongTensor([item[\"label\"] for item in batch])\n",
    "    return {\"vectors\": sentence_vector, \"labels\": labels, \"sentences\": [item[\"sentence\"] for item in batch]}\n",
    "\n",
    "  # Training dataloader .. will reset itself each epoch\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.classification_train, batch_size=self.params.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "  # Validation dataloader .. will reset itself each epoch\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.classification_val, batch_size=self.params.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "  # Test dataloader .. will reset itself each epoch\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.classification_test, batch_size=self.params.batch_size, collate_fn=self.collate_fn)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W1XWmeAvqZ9"
   },
   "source": [
    "### Classifier and Trainer (Common to all solutions)\n",
    "\n",
    "We've now created the DataLoader and Datasets we'll use in the entire project. It is time to write the training and testing loops.\n",
    "\n",
    "[LightingModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html): organizes your PyTorch code into 5 sections\n",
    "\n",
    "1. Computations (init).\n",
    "2. Train loop (training_step)\n",
    "3. Validation loop (validation_step)\n",
    "4. Test loop (test_step)\n",
    "5. Optimizers (configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BRgqB5A_vpiT"
   },
   "source": [
    "# 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "class EmotionClassifier(pl.LightningModule):\n",
    "  def __init__(self, model, params):\n",
    "      super().__init__()\n",
    "      self.model = model\n",
    "      self.params = params\n",
    "      self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=params.num_classes)\n",
    "\n",
    "  # will be called automatically by \"training_step\", \"validation_step\", etc.\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"labels\"]\n",
    "    y_hat = self(x)\n",
    "    loss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    self.log_dict(\n",
    "        {'train_loss': loss}, \n",
    "        batch_size=self.params.batch_size, \n",
    "        prog_bar=True\n",
    "        )\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, batch, batch_nb):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"labels\"]\n",
    "    y_hat = self(x)\n",
    "    val_loss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'val_loss': val_loss,\n",
    "          'val_accuracy': self.accuracy(predictions, y)\n",
    "        },\n",
    "        batch_size=self.params.batch_size,  \n",
    "        prog_bar=True\n",
    "      )\n",
    "    return val_loss\n",
    "\n",
    "  def test_step(self, batch, batch_nb):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"labels\"]\n",
    "    y_hat = self(x)\n",
    "    test_loss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'test_loss': test_loss,\n",
    "          'test_accuracy': self.accuracy(predictions, y)\n",
    "        },\n",
    "        batch_size=self.params.batch_size, \n",
    "        prog_bar=True\n",
    "      )\n",
    "    return test_loss\n",
    "  \n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[\"vectors\"])\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    return {'logits':y_hat, 'predictions': predictions, 'labels': batch[\"labels\"], 'sentences': batch['sentences']}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.params.learning_rate)\n",
    "    return optimizer"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have a Lightning and LightingDataModule, a [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) automates everything else. It provides functions for training (fit), testing and inference. We ended up writing a helper function that takes the model, vectorizer and hyper parameters to be able to easily compare our different models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class WordVectorClassificationModel(torch.nn.Module):\n",
    "  def __init__(self, word_vec_dimension, num_classes):\n",
    "    super().__init__()\n",
    "    self.classes = num_classes\n",
    "    self.linear_layer = torch.nn.Linear(word_vec_dimension, num_classes)\n",
    "    \n",
    "  # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "  def forward(self, batch):\n",
    "    \"\"\"Projection from word_vec_dim to n_classes\n",
    "\n",
    "    Batch is of shape (batch_size, max_seq_len, word_vector_dim)\n",
    "    \"\"\"\n",
    "    return self.linear_layer(batch)"
   ],
   "metadata": {
    "id": "nV5_OxBQOhRq"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def trainer(model, params, vectorizer):\n",
    "  # Create a pytorch trainer\n",
    "  trainer = pl.Trainer(max_epochs=params.max_epochs, check_val_every_n_epoch=1)\n",
    "\n",
    "  # Initialize our data loader with the passed vectorizer\n",
    "  data_module = ClassificationDataModule(vectorizer, params)\n",
    "\n",
    "  # Instantiate a new model\n",
    "  emotionClassifier = EmotionClassifier(model, params)\n",
    "\n",
    "  # Train and validate the model\n",
    "  trainer.fit(emotionClassifier, data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader())\n",
    "\n",
    "  # Test the model\n",
    "  trainer.test(emotionClassifier, data_module.test_dataloader())\n",
    "\n",
    "  # Predict on the same test set to show some output\n",
    "  output = trainer.predict(emotionClassifier, data_module.test_dataloader())\n",
    "\n",
    "  for i in range(2):\n",
    "    print(\"-----------\")\n",
    "    print(\"Sentence: \", output[1]['sentences'][i])\n",
    "    print(\"Predicted Emotion: \", integer_to_label[output[1]['predictions'][i].item()])\n",
    "    print(\"Actual Label: \", integer_to_label[output[1]['labels'][i].item()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "\n",
    "Are we building models yet? Finally the time has come to build our baseline model and then we'll work towards improving it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 1: Average word vector of the sentence -- Baseline\n",
    "##### <font color='red'>Expected accuracy: ~29 - 32%</font>\n",
    "\n",
    "Let's build our first simple word2vec based model we'll use as our baseline.\n",
    "\n",
    "Here we've three key pieces:\n",
    "\n",
    "1. *WordVectorClassificationModel*: Simple linear model that just has one single neuron layer that maps the input word2vec dimensions (300) to the output classes (32) building a really simple classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. *HParams*: a class that contains all the hyper parameters relevant for the current model.\n",
    "3. *SpacyVectorizer*: Vectorizer that converts the text sentence into the input to the DataLoader's collate function. Basically we'll call vectorizer on each row of the input data and then call the collate_fn on each batch of items which is fed to the Neural Network.\n",
    "\n",
    "It will take several minutes to train the model, so don't be alarmed if you don't get the result right away. When the cell finishes running, under the `DATALOADER:0 TEST RESULTS` section, you should see the `test_accuracy` field with a value of ~0.3."
   ],
   "metadata": {
    "id": "JOEdZjITP_No"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0MaFrXauuPnj"
   },
   "source": [
    "class HParams:\n",
    "  batch_size: int = 32\n",
    "  integer_input: bool = False\n",
    "  # cannot change, because en_core_web_md creates word vectors with 300 dimensions\n",
    "  word_vec_dimension: int = 300\n",
    "  num_classes: int = 32\n",
    "  learning_rate: float = 0.001\n",
    "  max_epochs: int = 4\n",
    "\n",
    "\n",
    "# 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "class SpacyVectorizer:\n",
    "  def vectorize(self, sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, tokenize it and reference pre-trained word vector for each token.\n",
    "\n",
    "    Returns a tuple of sentence_vector and list of text tokens\n",
    "    \"\"\"\n",
    "    sentence_vector = []\n",
    "    sentence_tokens = []\n",
    "    # https://spacy.io/api/language#attributes\n",
    "    spacy_doc = loaded_spacy_model.make_doc(sentence) ## I am Sourabh\n",
    "    word_vector = [token.vector for token in spacy_doc] ## [ [Embedding of I], [Embedding of am], [Embedding of UNK]] \n",
    "    sentence_tokens = list([token.text for token in spacy_doc]) # [[I], [am], [Sourabh]]\n",
    "    sentence_vector = np.mean(np.array(word_vector), axis=0)\n",
    "    return sentence_vector, sentence_tokens\n",
    "\n",
    "# default 4 epoch\n",
    "# test_accuracy       0.3255000114440918\n",
    "# test_loss           2.4564757347106934\n",
    "\n",
    "# 1 epoch\n",
    "# test_accuracy       0.24950000643730164\n",
    "# test_loss           2.930525302886963\n",
    "\n",
    "# 20 epoch\n",
    "# test_accuracy       0.3894999921321869\n",
    "# test_loss           2.1219022274017334\n",
    "trainer(\n",
    "    model=WordVectorClassificationModel(HParams.word_vec_dimension,\n",
    "                                        HParams.num_classes),\n",
    "    params=HParams,\n",
    "    vectorizer=SpacyVectorizer())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_ugsal9dhVr"
   },
   "source": [
    "🎉🎉🎉 WE HAVE OUR TEXT CLASSIFIER 🎉🎉🎉\n",
    "\n",
    "\n",
    "Now might be a good time to play around with the vectorizer and the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNvtK1-BJEq-"
   },
   "source": [
    "### Assignment Part: 1 - Model 2: Sliding Window Word2Vec ---- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~30 to 36%</font>\n",
    "\n",
    "We'll be re-using the simple linear model from Model-1 but changing the input to use sliding windows instead of one word at a time. \n",
    "\n",
    "Implement a new `SpacyChunkVectorizer` which is a variant of the `SpacyVectorizer` that operates. Here are some instructions on how to implement it.\n",
    "\n",
    "1. Split the sentence into chunks of the size of the n_grams parameter.\n",
    "2. Concat all the spacy embeddings of the tokens inside to create embeddings of the chuck. Each chunk vector is of the size of `3 * size_of_embedding`\n",
    "3. Sentence vector is the average of all chunk vectors.\n",
    "4. Return the sentence_vector and tokens (for debugging, option: could just return None for tokens)\n",
    "\n",
    "Does this model perform better than our baseline? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nuJFhiRN7654"
   },
   "source": [
    "class HParamsSpacy:\n",
    "  batch_size: int = 32\n",
    "  integer_input: bool = False\n",
    "  word_vec_dimension: int = 300\n",
    "  num_classes: int = 32\n",
    "  learning_rate: float = 0.001\n",
    "  max_epochs: int = 4\n",
    "  n_grams: int = 3  ## ADAPT, Change it to your liking\n",
    "\n",
    "\n",
    "class SpacyChunkVectorizer:\n",
    "  def __init__(self, params):\n",
    "    self.params = params\n",
    "\n",
    "  def vectorize(self, sentence):\n",
    "    \"\"\"Given a sentence, tokenize it and returns a word vector for that sentence.\n",
    "\n",
    "    Sentence is of length (n)\n",
    "\n",
    "    1. Split the sentence into tokens using Spacy's function make_doc\n",
    "    2. Split the list of token into size of the n_grams parameter.\n",
    "    3. Concat all the spacy embeddings of the tokens inside to create embeddings of the chuck. Each chunk vector is of the size of 3 * size_of_embedding\n",
    "    4. Sentence vector is the average of all chunk vectors.\n",
    "    5. Return the sentence_vector and tokens (option: could just return None for tokens)\n",
    "\n",
    "    Sentence_vector is of length (n_grams * word_vector_dim)\n",
    "    Sentence_tokens is of length (n)\n",
    "\n",
    "    Example of word tri-gram encoding: \"I am doing great right now.\":\n",
    "      <EMPTY (300,)> <EMPTY (300,)> <I (300,)> -> (900, )\n",
    "      <EMPTY (300,)> <I (300,)> <am (300,)> -> (900, )\n",
    "      <I (300,)> <am (300,)> <doing (300,)> -> (900, )\n",
    "      <am (300,)> <doing (300,)> <great (300,)> -> (900, )\n",
    "      <doing (300,)> <great (300,)> <right (300,)> -> (900, )\n",
    "      <great (300,)> <right (300,)> <now (300,)> -> (900, )\n",
    "      <right (300,)> <now (300,)> <EMPTY (300,)> -> (900, )\n",
    "      <now (300,)> <EMPTY (300,)> <EMPTY (300,)> -> (900, )\n",
    "\n",
    "    We'd encourage you to also try other variants to encode!\n",
    "    \"\"\"\n",
    "    ### +TO BE IMPLEMENTED ###\n",
    "\n",
    "    # https://spacy.io/api/doc/\n",
    "    spacy_doc = loaded_spacy_model.make_doc(sentence) ## I am Sourabh\n",
    "    word_tokens = [token for token in spacy_doc] ## [ [Embedding of I], [Embedding of am], [Embedding of UNK]]\n",
    "    sentence_tokens = [] # [\"text1 text2 text3\", \"text2 text3 text4\"]\n",
    "    chunk_vectors = []\n",
    "\n",
    "    # fill in gaps if there are not enough tokens for one window\n",
    "    if len(word_tokens) < self.params.n_grams:\n",
    "      while len(word_tokens) != self.params.n_grams:\n",
    "        empty_token = loaded_spacy_model.make_doc('')\n",
    "        word_tokens.append(empty_token)\n",
    "\n",
    "    head = 0\n",
    "    while head + self.params.n_grams - 1  <= len(word_tokens)-1:\n",
    "      window_tokens = [] # [[I], [am], [Sourabh]]\n",
    "      for i in range(head, head + self.params.n_grams):\n",
    "        window_tokens.append(word_tokens[i])\n",
    "\n",
    "      window_texts = [token.text for token in window_tokens]\n",
    "      window_vectors = [token.vector for token in window_tokens]\n",
    "\n",
    "      window_texts_concatenated = ' '.join(window_texts)\n",
    "      window_vectors_concatenated = np.concatenate(np.array(window_vectors))\n",
    "\n",
    "      sentence_tokens.append(window_texts_concatenated)\n",
    "      chunk_vectors.append(window_vectors_concatenated)\n",
    "\n",
    "      head=head+1\n",
    "\n",
    "    sentence_vector = np.mean(np.array(chunk_vectors), axis=0)\n",
    "\n",
    "    return sentence_vector, sentence_tokens\n",
    "\n",
    "\n",
    "# 1 token in window\n",
    "# test_accuracy       0.3255000114440918\n",
    "\n",
    "# 2 tokens in window\n",
    "# test_accuracy       0.3542500138282776\n",
    "# test_loss           2.2810938358306885\n",
    "\n",
    "# 3 tokens in window\n",
    "# test_accuracy       0.3644999861717224\n",
    "# test_loss           2.2147631645202637\n",
    "\n",
    "# 6 tokens in window\n",
    "# test_accuracy       0.37299999594688416\n",
    "# test_loss           2.184687852859497\n",
    "\n",
    "# BEST: 6 tokens in window + 20 epoch\n",
    "# test_accuracy       0.390500009059906\n",
    "# test_loss           2.1243062019348145\n",
    "\n",
    "# 10 tokens in window\n",
    "# test_accuracy       0.367249995470047\n",
    "# test_loss           2.19740629196167\n",
    "\n",
    "trainer(\n",
    "    model=WordVectorClassificationModel(\n",
    "        # Observe the change in input parameters\n",
    "        HParamsSpacy.word_vec_dimension * HParamsSpacy.n_grams,  \n",
    "        HParamsSpacy.num_classes),\n",
    "    params=HParamsSpacy,\n",
    "    vectorizer=SpacyChunkVectorizer(HParamsSpacy))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L37awShJH09"
   },
   "source": [
    "### Assignment Part: 2 - Model 3: EmbeddingBag  ---- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~32 to 38%</font>\n",
    "\n",
    "The third model we're going to build is an embedding layer based model. Here instead of using pre-trained word-embeddings we'll be creating new vectors as part of the training process. How do you think this model will perform?\n",
    "\n",
    "Implementation has the following steps:\n",
    "\n",
    "1. **`get_char_trigram_token_map`**: Implement a map that returns top `num_tokens` token in the corpus to some allocated ids between `1 to num_tokens`. Here are some steps that should help with the implementation.\n",
    "      1. Compute a frequency map of the `num_tokens` most common  character trigrams in the training data. **Note: We're now moving away from words and moving to just using characters directly**.\n",
    "      3. Create unique integer ids for all these tokens 1...N\n",
    "\n",
    "2. **Vectorizer**: Implement a new vectorizer for each sentence that does the following:\n",
    "    1. Get all trigrams for the sentence\n",
    "    2. Get id for every trigram\n",
    "    4. Append all the ids into a list and that is your sentence vector\n",
    "\n",
    "3. **Forward pass of the model**: Implement the forward pass of the model\n",
    "    1. Pass the input batch through the embedding layer\n",
    "    1. Pass the output of the embedding layer into our linear layer  \n",
    "\n",
    "\n",
    "Some rational for trying this out is:\n",
    "- We average the embeddings in a sentence anyway so word or not word maybe doesn't matter.\n",
    "- Vocabulary of character trigrams is much smaller than word trigrams so our models are easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class HParamsCTT:\n",
    "  batch_size: int = 16  ### NOTE THE CHANGE\n",
    "  integer_input: bool = True  ### NOTE THE CHANGE\n",
    "  num_classes: int = 32\n",
    "  learning_rate: float = 0.001\n",
    "  max_epochs: int = 4\n",
    "  n_grams: int = 3\n",
    "  embed_dim: int = 350 # ADDED, Change it to your liking\n",
    "  num_tokens: int = 5000\n",
    "\n",
    "\n",
    "class CharacterTrigramTokenizer:\n",
    "  \"\"\"\n",
    "  We represent a sentence as a vector of num_tokens tokens.\n",
    "  If the trigram is present in the sentence then we add the token's id to the sentence\n",
    "  \"\"\"\n",
    "  def __init__(self, train_data, num_tokens):\n",
    "    self.num_tokens = num_tokens\n",
    "    self.token_to_id_map = self.get_char_trigram_token_map(train_data, num_tokens)\n",
    "\n",
    "  def get_char_trigram_token_map(self, train_data, num_tokens):\n",
    "    \"\"\"\n",
    "    1. Compute a frequency map of the `num_tokens` most common trigrams in the training data.\n",
    "    2. Create unique integer ids for all these tokens 1...N\n",
    "    \"\"\"\n",
    "\n",
    "    ### + TO BE IMPLEMENTED ###\n",
    "    token_to_id_map = {}\n",
    "\n",
    "    number_of_characters_in_gram = 3\n",
    "    trigram_to_count_map = self.get_trigram_to_count_map(train_data, number_of_characters_in_gram)\n",
    "\n",
    "    sorted_list = sorted(trigram_to_count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_gram =  sorted_list[:num_tokens]\n",
    "    for i, (trigram, count) in enumerate(top_gram):\n",
    "      token_to_id_map[trigram] = i\n",
    "    token_to_id_map['<unk>'] = num_tokens + 1\n",
    "\n",
    "    return token_to_id_map\n",
    "\n",
    "  def get_trigram_to_count_map(self, train_data, number_of_characters_in_gram):\n",
    "    gram_to_count_map = {}\n",
    "\n",
    "    padding = '  '\n",
    "    for sample in train_data:\n",
    "      sentence = sample[1]\n",
    "      sentence_lower = padding + sentence.lower() + padding\n",
    "\n",
    "      for i in range(len(sentence_lower) - (number_of_characters_in_gram - 1)):\n",
    "        trigram = sentence_lower[i:i+number_of_characters_in_gram]\n",
    "\n",
    "        if trigram not in gram_to_count_map:\n",
    "          gram_to_count_map[trigram] = 0\n",
    "        gram_to_count_map[trigram] = gram_to_count_map[trigram] + 1\n",
    "\n",
    "    return gram_to_count_map\n",
    "\n",
    "\n",
    "  def vectorize(self, sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence (string), do the following -\n",
    "    1. Get all trigrams for the sentence\n",
    "    2. Get id for every trigram\n",
    "    3. Append all the ids into a list and that is your sentence vector\n",
    "\n",
    "    Example of the char tri-gram encoding: \"How's going?\":\n",
    "      <EMPTY (300,)> <EMPTY (300,)> <H (300,)> -> (900, )\n",
    "      <EMPTY (300,)> <H (300,)> <o (300,)> -> (900, )\n",
    "      <H (300,)> <o (300,)> <w (300,)> -> (900, )\n",
    "      <o (300,)> <w (300,)> <' (300,)> -> (900, )\n",
    "      <w (300,)> <' (300,)> <s (300,)> -> (900, )\n",
    "      <' (300,)> <s (300,)> <  (300,)> -> (900, )\n",
    "      <s (300,)> <  (300,)> <g  (300,)> -> (900, )\n",
    "      <  (300,)> <g (300,)> <o  (300,)> -> (900, )\n",
    "      <g (300,)> <o (300,)> <i  (300,)> -> (900, )\n",
    "      <o (300,)> <i (300,)> <n  (300,)> -> (900, )\n",
    "      <i (300,)> <n (300,)> <g  (300,)> -> (900, )\n",
    "      <n (300,)> <g (300,)> <?  (300,)> -> (900, )\n",
    "      <g (300,)> <? (300,)> <EMPTY (300,)> -> (900, )\n",
    "      <? (300,)> <EMPTY (300,)> <EMPTY (300,)> -> (900, )\n",
    "\n",
    "    We'd encourage you to also try other variants to encode!\n",
    "    \"\"\"\n",
    "    sentence_vector = []\n",
    "\n",
    "    ### + TO BE IMPLEMENTED ###\n",
    "    padding = '  '\n",
    "    sentence_lower = padding + sentence.lower() + padding\n",
    "    n_grams = 3\n",
    "    unk = self.token_to_id_map['<unk>']\n",
    "    for i in range(len(sentence_lower) - (n_grams - 1)):\n",
    "      trigram = sentence_lower[i:i+n_grams]\n",
    "      sentence_vectsor.append(self.token_to_id_map.get(trigram, unk))\n",
    "\n",
    "    len(sentence_vector)\n",
    "    return sentence_vector, None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3yv54UAkahv"
   },
   "source": [
    "Let's just validate the output of the tokenizer before we train the model. We should see something like:\n",
    "\n",
    "```\n",
    " I  1\n",
    " th 2\n",
    "the 3\n",
    " to 4\n",
    "ing 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "([5001, 1725, 5001, 378, 378], None)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characterTokenizer = CharacterTrigramTokenizer(training_data, HParamsCTT.num_tokens)\n",
    "\n",
    "characterTokenizer.vectorize(\"1\")\n",
    "characterTokenizer.vectorize(\"2 \")\n",
    "characterTokenizer.vectorize(\"3  \")\n",
    "# i = 0\n",
    "# for k, v in characterTokenizer.token_to_id_map.items():\n",
    "#   if i >= 5:\n",
    "#     break\n",
    "#   print(k,v)\n",
    "#   i += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXLY0jWgkhv8"
   },
   "source": [
    "Now we can create the simple embedding layer based model and start training it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eix7Svdr-r3P"
   },
   "source": [
    "\n",
    "# torch.nn.Module - Base class for all neural network modules.\n",
    "\n",
    "class EmbeddingBagClassificationModel(torch.nn.Module):\n",
    "  def __init__(self, num_tokens, embed_dim, n_classes):\n",
    "    super().__init__()\n",
    "    self.classes = n_classes\n",
    "    self.embedding = torch.nn.EmbeddingBag(num_tokens, embed_dim)\n",
    "    self.linear_layer = torch.nn.Linear(embed_dim, n_classes)\n",
    "    \n",
    "  def forward(self, batch):\n",
    "    \"\"\"Pass the input batch through the embedding layer and then follow it up with the linear layer\n",
    "    \"\"\"\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    y = self.linear_layer(self.embedding(batch))\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "trainer(\n",
    "    # Note the plus one: EmbeddingBag has 5000 + 1 tokens, where the +1 is\n",
    "    # the padding embedding, which is a zero-vector.\n",
    "    model=EmbeddingBagClassificationModel(\n",
    "        HParamsCTT.num_tokens + 1, \n",
    "        HParamsCTT.embed_dim, \n",
    "        HParamsCTT.num_classes), \n",
    "    params=HParamsCTT,\n",
    "    vectorizer=characterTokenizer)"
   ],
   "execution_count": 96,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type                            | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model    | EmbeddingBagClassificationModel | 1.8 M \n",
      "1 | accuracy | MulticlassAccuracy              | 0     \n",
      "-------------------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88d279eb5b234ed5bfb3688320a7bf79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected i < index_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/ty/jsgf4tvx4gngtpn4j4m7dc280000gp/T/ipykernel_93350/2037881172.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m         HParamsCTT.num_classes), \n\u001B[1;32m     27\u001B[0m     \u001B[0mparams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mHParamsCTT\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m     vectorizer=characterTokenizer)\n\u001B[0m",
      "\u001B[0;32m/var/folders/ty/jsgf4tvx4gngtpn4j4m7dc280000gp/T/ipykernel_93350/1341798499.py\u001B[0m in \u001B[0;36mtrainer\u001B[0;34m(model, params, vectorizer)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m   \u001B[0;31m# Train and validate the model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m   \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memotionClassifier\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mval_dataloader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m   \u001B[0;31m# Test the model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    769\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m         self._call_and_handle_interrupt(\n\u001B[0;32m--> 771\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_impl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatamodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    772\u001B[0m         )\n\u001B[1;32m    773\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    721\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlauncher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlaunch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainer_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    722\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 723\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mtrainer_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    724\u001B[0m         \u001B[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    725\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mckpt_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_provided\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_connected\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m         )\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mckpt_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1234\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_checkpoint_connector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresume_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1235\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1236\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1237\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1238\u001B[0m         \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetail\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1321\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredicting\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1322\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1323\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1324\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1325\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_training_routine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1343\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1344\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0misolate_rng\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1345\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_sanity_check\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1346\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1347\u001B[0m         \u001B[0;31m# enable train mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1411\u001B[0m             \u001B[0;31m# run eval step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1412\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1413\u001B[0;31m                 \u001B[0mval_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1414\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1415\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_callback_hooks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"on_sanity_check_end\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_dataloaders\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"dataloader_idx\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataloader_idx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m         \u001B[0mdl_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepoch_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_fetcher\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdl_max_batches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    156\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[0;31m# store batch level output per dataloader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[0;31m# lightning module methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_evaluation_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_evaluation_step_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001B[0m in \u001B[0;36m_evaluation_step\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_strategy_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"test_step\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 226\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_strategy_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"validation_step\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1764\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1765\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1766\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1767\u001B[0m         \u001B[0;31m# restore current_fx when nested context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\u001B[0m in \u001B[0;36mvalidation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    342\u001B[0m         \"\"\"\n\u001B[1;32m    343\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mval_step_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 344\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidation_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    345\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    346\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtest_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mSTEP_OUTPUT\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/ty/jsgf4tvx4gngtpn4j4m7dc280000gp/T/ipykernel_93350/3462536794.py\u001B[0m in \u001B[0;36mvalidation_step\u001B[0;34m(self, batch, batch_nb)\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"vectors\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"labels\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m     \u001B[0my_hat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m     \u001B[0mval_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcross_entropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'mean'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/ty/jsgf4tvx4gngtpn4j4m7dc280000gp/T/ipykernel_93350/3462536794.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      9\u001B[0m   \u001B[0;31m# will be called automatically by \"training_step\", \"validation_step\", etc.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/ty/jsgf4tvx4gngtpn4j4m7dc280000gp/T/ipykernel_93350/2037881172.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \"\"\"\n\u001B[1;32m     13\u001B[0m     \u001B[0;31m### TO BE IMPLEMENTED ###\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m     \u001B[0;31m### TO BE IMPLEMENTED ###\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, offsets, per_sample_weights)\u001B[0m\n\u001B[1;32m    389\u001B[0m                                \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale_grad_by_freq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m                                \u001B[0mper_sample_weights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minclude_last_offset\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 391\u001B[0;31m                                self.padding_idx)\n\u001B[0m\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36membedding_bag\u001B[0;34m(input, weight, offsets, max_norm, norm_type, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx)\u001B[0m\n\u001B[1;32m   2391\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2392\u001B[0m     ret, _, _, _ = torch.embedding_bag(\n\u001B[0;32m-> 2393\u001B[0;31m         \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moffsets\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_grad_by_freq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode_enum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msparse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mper_sample_weights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minclude_last_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2394\u001B[0m     )\n\u001B[1;32m   2395\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected i < index_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZvWAI7soxvP"
   },
   "source": [
    "🎉 CONGRATS!!! on finishing the assignment. Now is a good time to pause and reflect how much progress we've made in understanding word vectors, reading some pytorch code and build our first model. But hey, don't stop here, there is a lot to do or play with in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM0pHSbtGHuj"
   },
   "source": [
    "# Extensions\n",
    "\n",
    "Now that you've worked through the project. There is a lot more for us to try.\n",
    "\n",
    "- Which model performed the best? Why do you think that was?\n",
    "- Try decreasing and increasing the size of the dataset. How does that impact training time and accuracy of each model?\n",
    "- Try adding a hidden layer to the baseline of the models and see if that changes anything\n",
    "- Does adding a hidden layer to the embedding bag model help?\n",
    "- visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#confusion-matrix-func) of N*N of actual class vs predicted class (N = number of classes)"
   ]
  }
 ]
}
