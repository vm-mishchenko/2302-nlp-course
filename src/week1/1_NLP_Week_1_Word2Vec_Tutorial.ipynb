{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziQY_aIHBcLR"
   },
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "In this notebook we're going to learn and play around with word2vec embeddings that are packaged with Spacy. We'll try to build intuition on how they work and what can we do with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2T-deBX3huM"
   },
   "source": [
    "Install all the required dependencies for the project"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uj-w6eV7_abU"
   },
   "source": [
    "%%capture\n",
    "!pip install spacy==2.2.4 --quiet\n",
    "!python -m spacy download en_core_web_md\n",
    "!apt install libopenblas-base libomp-dev\n",
    "!pip install faiss-cpu"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnFmq6hM3n4D"
   },
   "source": [
    "Import all the necessary libaries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NSDbwEoL8-0r"
   },
   "source": [
    "from collections import defaultdict\n",
    "import en_core_web_md\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import faiss"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQa8P1KPt4xP"
   },
   "source": [
    "Now let's load the Spacy data which comes with pre-trained embeddings. This process is expensive so only do this once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SA_Xwhmnt2Ph",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "outputId": "94cb7b52-7228-4bff-dcf7-0ddb554008ab"
   },
   "source": [
    "spacyModel = en_core_web_md.load()"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Vnj8WIrujBZ"
   },
   "source": [
    "First, let's play with some basic similarity functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DnRogtIDuekU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "57a45fc3-f20b-4daf-9499-321e603001c5"
   },
   "source": [
    "# API can be found here: https://spacy.io/api/doc\n",
    "banana = spacyModel(\"girl\")\n",
    "fruit = spacyModel(\"women\")\n",
    "table = spacyModel(\"table\")\n",
    "print(banana.similarity(fruit))\n",
    "print(banana.similarity(table))\n",
    "\n"
   ],
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5421239860059744\n",
      "0.2619973944685995\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp361KZ32XUR"
   },
   "source": [
    "As expected `Banana` is a lot more similar to `Fruit` than to `Table`. Now let's iterate over the entire vocabulary and build a search index using **Faiss**. This will make it a lot faster for us to find similar words instead of having to loop over the entire corpus each time. \n",
    "\n",
    "Feel free to ignore learning more about **Faiss** at this time as we'll dive more into it in Week 3. At the high-level it is a really efficient library to find similar vectors from a corpus.\n",
    "\n",
    "Note: This next cell will take a fair bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nNKVP0eYBbZm"
   },
   "source": [
    "def load_index():\n",
    "  \"\"\"Expensive method - call only once!!\n",
    "  \"\"\"\n",
    "  word_to_id = {}\n",
    "  id_to_word = {}\n",
    "  vectors = []\n",
    "  vector_dimension = 300\n",
    "  id = 0\n",
    "\n",
    "  # Iterate over the entire vocabulary\n",
    "  for i, tok in enumerate(spacyModel.vocab):\n",
    "    vector = tok.vector\n",
    "    l2_norm = np.linalg.norm(vector)\n",
    "\n",
    "    # Ignore zero vectors, nan vlaues\n",
    "    if (np.isclose(l2_norm, 0.0) or \n",
    "        np.isnan(l2_norm) or \n",
    "        np.any(np.isnan(vector))):\n",
    "      continue\n",
    "    else:\n",
    "      vectors.append(np.divide(vector, l2_norm))\n",
    "\n",
    "    # Add to the output variables\n",
    "    word_to_id[tok.text.lower()] = id\n",
    "    id_to_word[id] = tok.text.lower()\n",
    "    id += 1\n",
    "\n",
    "  \n",
    "  vectors = np.array(vectors)\n",
    "  index = faiss.IndexFlatIP(vector_dimension)\n",
    "  index.add(vectors)\n",
    "  return word_to_id, id_to_word, vectors, index\n",
    "\n",
    "word_to_id, id_to_word, vectors, index = load_index()\n",
    "vector_size = len(vectors)\n",
    "print(\"We created a search index of %d vectors\" % vector_size)"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We created a search index of 684754 vectors\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAQitr3R4bLA"
   },
   "source": [
    "Now we're going to add a helper functions to calculate top_k similar words to some input in the index."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n8rGyqh1F5O6"
   },
   "source": [
    "def search_vector(word_vector, top_k=100, print_time_taken=False):\n",
    "  word_vector = np.array([word_vector])\n",
    "  start_time = time.time()\n",
    "  scores, indices = index.search(word_vector, top_k)\n",
    "  if print_time_taken:\n",
    "    print(\"Time taken to search amongst {} words is {:.3}s\".format(\n",
    "        vector_size, (time.time() - start_time))\n",
    "    )\n",
    "  results = []\n",
    "  words = set()\n",
    "  for i, query_index in enumerate(indices):\n",
    "      # Matches for the i'th one \n",
    "      for inn_idx, word_index in enumerate(query_index):\n",
    "          if word_index < 0:\n",
    "              continue\n",
    "          word = id_to_word[word_index]\n",
    "          if word in words:\n",
    "            continue\n",
    "          words.add(word)\n",
    "          results.append((word, float(scores[i][inn_idx])))\n",
    "  return sorted(results, key=lambda tup: -tup[1])"
   ],
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcmoOvfA54JS"
   },
   "source": [
    "Let's do an empirical test by searching similar words to a few terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSriAnCr9g7c"
   },
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LaaWpQ97oCre"
   },
   "source": [
    "def search(word, top_k=100,print_time_taken=False):\n",
    "  word = word.lower()\n",
    "  if word not in word_to_id:\n",
    "    print(\"Oops, the word {} is not in loaded dictionary\".format(word))\n",
    "    return\n",
    "  id = word_to_id[word]\n",
    "  word_vector = vectors[id]\n",
    "  search_results = search_vector(word_vector, top_k, print_time_taken)\n",
    "  print(f\"The top similar words to {word} are - \")\n",
    "  for i in range(len(search_results)):\n",
    "    print(f\"Word = {search_results[i][0]} and similarity = {search_results[i][1]}\")\n",
    "  return search_results"
   ],
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f6iSiVNv7A6D"
   },
   "source": [
    "output = search(\"portland\", 100, True)"
   ],
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to search amongst 684754 words is 0.0355s\n",
      "The top similar words to portland are - \n",
      "Word = beaverton and similarity = 1.0\n",
      "Word = pdx and similarity = 1.0\n",
      "Word = pdxpipeline and similarity = 1.0\n",
      "Word = sellwood and similarity = 1.0\n",
      "Word = portland and similarity = 1.0\n",
      "Word = sammamish and similarity = 1.0\n",
      "Word = milwaukie and similarity = 1.0\n",
      "Word = seatac and similarity = 0.8688419461250305\n",
      "Word = redmond and similarity = 0.8688419461250305\n",
      "Word = belltown and similarity = 0.8688419461250305\n",
      "Word = safeco and similarity = 0.8688419461250305\n",
      "Word = tukwila and similarity = 0.8688419461250305\n",
      "Word = lynnwood and similarity = 0.8688419461250305\n",
      "Word = showbox and similarity = 0.8688419461250305\n",
      "Word = bothell and similarity = 0.8688419461250305\n",
      "Word = renton and similarity = 0.8688419461250305\n",
      "Word = mukilteo and similarity = 0.8688419461250305\n",
      "Word = bellevue and similarity = 0.8688419461250305\n",
      "Word = seattle and similarity = 0.8688419461250305\n",
      "Word = poulsbo and similarity = 0.8688419461250305\n",
      "Word = issaquah and similarity = 0.8688419461250305\n",
      "Word = bremerton and similarity = 0.8688419461250305\n",
      "Word = puget and similarity = 0.8688419461250305\n",
      "Word = burien and similarity = 0.8688419461250305\n",
      "Word = kirkland and similarity = 0.8688419461250305\n",
      "Word = puyallup and similarity = 0.8688419461250305\n",
      "Word = olympia and similarity = 0.8688419461250305\n",
      "Word = greenlake and similarity = 0.8688419461250305\n",
      "Word = bellingham and similarity = 0.8688419461250305\n",
      "Word = tacoma and similarity = 0.8688419461250305\n",
      "Word = seatle and similarity = 0.8688419461250305\n",
      "Word = windermere and similarity = 0.8688419461250305\n",
      "Word = medford and similarity = 0.8265719413757324\n",
      "Word = canby and similarity = 0.8265719413757324\n",
      "Word = assn and similarity = 0.8265719413757324\n",
      "Word = gresham and similarity = 0.8265719413757324\n",
      "Word = humboldt and similarity = 0.8265719413757324\n",
      "Word = oregon and similarity = 0.8265719413757324\n",
      "Word = chelan and similarity = 0.8265719413757324\n",
      "Word = dalles and similarity = 0.8265719413757324\n",
      "Word = brookings and similarity = 0.8265719413757324\n",
      "Word = tualatin and similarity = 0.8265719413757324\n",
      "Word = ashland and similarity = 0.8265719413757324\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ni0bFOxi7CFY"
   },
   "source": [
    "output = search(\"baseball\", 10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8IzaN0X67M70"
   },
   "source": [
    "output = search(\"cheese\", 25)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovIxI9Uutgc-"
   },
   "source": [
    "Now why don't you try out a few different words that come to mind and see where does the model perform well and where it struggles!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVPJ2_R59i-2"
   },
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bunCp7p9rplw"
   },
   "source": [
    "def analogy(word1, word2, word3):\n",
    "  word1 = word1.lower()\n",
    "  word2 = word2.lower()\n",
    "  word3 = word3.lower()\n",
    "  if word1 not in word_to_id or word2 not in word_to_id or word3 not in word_to_id:\n",
    "    print(\"word not present in dictionary, try something else\")\n",
    "  vector1 = vectors[word_to_id[word1]]\n",
    "  vector2 = vectors[word_to_id[word2]]\n",
    "  vector3 = vectors[word_to_id[word3]]\n",
    "  analogy_results = search_vector(np.add(np.subtract(vector1, vector2), vector3), 10)\n",
    "  print(f\"The top similar item for ({word1} - {word2} + {word3}) = {analogy_results[0][0]}\")\n",
    "  print(f\"The top similar words to ({word1} - {word2} + {word3}) are - \")\n",
    "  for i in range(len(analogy_results)):\n",
    "    print(f\"Word = {analogy_results[i][0]} and similarity = {analogy_results[i][1]}\")\n",
    "  return analogy_results"
   ],
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-N6uPUoQ5i15"
   },
   "source": [
    "output = analogy(\"test\", \"test\", \"test\")"
   ],
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top similar item for (test - test + test) = test\n",
      "The top similar words to (test - test + test) are - \n",
      "Word = test and similarity = 0.9999998807907104\n",
      "Word = retest and similarity = 0.9063977003097534\n",
      "Word = polygraph and similarity = 0.9063977003097534\n",
      "Word = pre-test and similarity = 0.9063977003097534\n",
      "Word = tests and similarity = 0.9063977003097534\n",
      "Word = lie-detector and similarity = 0.9063977003097534\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5XkbyKrc7ySI"
   },
   "source": [
    "output = analogy(\"smallest\", \"small\", \"short\")"
   ],
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top similar item for (smallest - small + short) = shortest\n",
      "The top similar words to (smallest - small + short) are - \n",
      "Word = shortest and similarity = 0.8045328259468079\n",
      "Word = straightest and similarity = 0.8045328259468079\n",
      "Word = sixth-longest and similarity = 0.7076637744903564\n",
      "Word = longest-running and similarity = 0.7076637744903564\n",
      "Word = third-longest and similarity = 0.7076637744903564\n",
      "Word = longest-ever and similarity = 0.7076637744903564\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4n1n870tqVS"
   },
   "source": [
    "Now why don't you try out a few different examples see what comes out :) "
   ]
  }
 ]
}
