{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfs1-EmH7ufz"
   },
   "source": [
    "# Week 4: Text Generation\n",
    "\n",
    "### What we are building\n",
    "A smart compose system that assists in writing movie reviews using the IMDB movie review dataset. FYI: You probably interact with smart compose multiple times a day while typing in Gmail, typing on your phone, or just using Google search.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "We will compare a really simple memorization model that just remembers how often certain words follow a phrase with a pre-trained GPT-2. Finetuning a GPT-2 can take a long time even with a GPU so we'll leave that as an extension project.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Datasets - Methods and dataset for evaluation\n",
    "- Models\n",
    "  - Memorization\n",
    "  - GPT-2 Pretrained\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8zLCEfd7VKI"
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "âœ¨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sdNdHsyAKXzZ"
   },
   "source": [
    "# Install all the required dependencies for the project\n",
    "!pip install transformers==4.17.0\n",
    "!pip install datasets==1.15.1\n",
    "!pip install pytorch-lightning==1.6.5"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.17.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (4.17.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (23.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (2022.10.31)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (6.0)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (2.28.2)\r\n",
      "Requirement already satisfied: sacremoses in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (0.0.53)\r\n",
      "Requirement already satisfied: filelock in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (3.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (1.21.6)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (0.13.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (6.0.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (4.64.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (0.12.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.5.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata->transformers==4.17.0) (3.13.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (3.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (3.4)\r\n",
      "Requirement already satisfied: six in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers==4.17.0) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers==4.17.0) (1.2.0)\r\n",
      "Requirement already satisfied: click in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers==4.17.0) (8.1.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting datasets==1.15.1\r\n",
      "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\r\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290 kB 2.7 MB/s            \r\n",
      "\u001B[?25hCollecting multiprocess\r\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\r\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115 kB 20.2 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: packaging in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (23.0)\r\n",
      "Collecting dill\r\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (0.12.1)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (4.64.1)\r\n",
      "Requirement already satisfied: pandas in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (1.1.5)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading xxhash-3.2.0-cp37-cp37m-macosx_10_9_x86_64.whl (34 kB)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (2023.1.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (3.8.4)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (6.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (1.21.6)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from datasets==1.15.1) (2.28.2)\r\n",
      "Collecting pyarrow!=4.0.0,>=1.0.0\r\n",
      "  Downloading pyarrow-11.0.0-cp37-cp37m-macosx_10_14_x86_64.whl (24.4 MB)\r\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.4 MB 13.1 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (22.2.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (1.3.1)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (0.13.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (3.0.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (4.0.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (1.8.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp->datasets==1.15.1) (1.3.3)\r\n",
      "Requirement already satisfied: filelock in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.9.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (6.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.15.1) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.15.1) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.15.1) (2022.12.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata->datasets==1.15.1) (3.13.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas->datasets==1.15.1) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas->datasets==1.15.1) (2022.7.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.15.1) (1.16.0)\r\n",
      "Installing collected packages: dill, xxhash, pyarrow, multiprocess, datasets\r\n",
      "Successfully installed datasets-1.15.1 dill-0.3.6 multiprocess-0.70.14 pyarrow-11.0.0 xxhash-3.2.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: pytorch-lightning==1.6.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.6.5)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.21.6)\r\n",
      "Requirement already satisfied: torch>=1.8.* in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.5.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (0.11.1)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (2023.1.0)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (2.11.2)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (0.3.2)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (23.0)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.64.1)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (3.20.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.8.4)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2.28.2)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.51.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.6.1)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.37.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.2.3)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.6)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.16.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (60.2.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.4.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.1)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (0.13.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (22.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.0.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.8.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (4.0.2)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (5.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (4.9)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.16.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (6.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.26.14)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.1.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.13.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.2.2)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BccBURgfO_3t"
   },
   "source": [
    "Import all the necessary libraries we need throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x4BCnkSaphtI"
   },
   "source": [
    "# Import all the relevant libraries\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy9Pc5zAv1KG"
   },
   "source": [
    "### Dataset Loading (common to all solutions)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ml67kfADUyHr"
   },
   "source": [
    "dataset_builder = load_dataset_builder('imdb')\n",
    "train_dataset = [d[\"text\"] for d in load_dataset('imdb', split='train')]\n",
    "test_dataset = [d[\"text\"] for d in load_dataset('imdb', split='test')]\n",
    "\n",
    "print(f\"Length of training data: {len(train_dataset)}\")\n",
    "print(f\"Length of test data: {len(test_dataset)}\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/vitalii.mishchenko/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/Users/vitalii.mishchenko/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 25000\n",
      "Length of test data: 25000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vZNSpB6PVHe"
   },
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "Running GPT-2 is really expensive so we create a small sample dataset of size 500 and use that for our evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jqGEabKMv4yZ"
   },
   "source": [
    "# Fix the random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "def create_eval_dataset(dataset, num_examples=500):\n",
    "  if len(dataset) < num_examples:\n",
    "    raise ValueError(f\"Can not select {num_examples} unique examples from dataset of size {len(dataset)}\")\n",
    "\n",
    "  # Since it is really expensive to run GPT, we'll use a smaller dataset for eval\n",
    "  sample = np.random.choice(dataset, num_examples, replace=False)\n",
    "\n",
    "  prefixes = []\n",
    "  output_words = []\n",
    "  for d in sample:\n",
    "    words = d.lower().split(\" \")\n",
    "    boundary = np.random.randint(1, len(words)-1)\n",
    "    prefix = \" \".join(words[:boundary])\n",
    "    prefixes.append(prefix)\n",
    "    output_words.append(words[boundary])\n",
    "  return prefixes, output_words\n",
    "\n",
    "prefixes, output_words = create_eval_dataset(test_dataset, 500)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC5yAi2oPsUl"
   },
   "source": [
    "**Evaluation Function**: Create a single function to compute correct predictions in the top_k from the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "csTmZ4MNFLsQ"
   },
   "source": [
    "def evaluate_exact_match_at(model, prefixes, output_words, top_k):\n",
    "  em_count = 0\n",
    "  i = 0\n",
    "  for i, (prefix, output_word) in enumerate(zip(prefixes, output_words)):\n",
    "    for p in model.predict(prefix, top_k):\n",
    "      if p.strip() == output_word.strip():\n",
    "        em_count += 1\n",
    "        break\n",
    "    if i % 20 == 0:\n",
    "      print(f\"Evaluated {i} prefixes\")\n",
    "  print(f\"Exact match evaluation em@{top_k}:{em_count /len(prefixes)} . Model got {em_count} matches out of {len(prefixes)}\")"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9idYvkY37zhJ"
   },
   "source": [
    "# Memorizer\n",
    "\n",
    "Model takes the largest prefix it will memorize which defaults to 3. This means for each sentence of the 4 words such as `I like learning NLP` it'll memorize that it saw `NLP` follow the prefix `I like learning` once.  \n",
    "\n",
    "The model also memorizes any window of size between 1 to the largest_prefix length that are fall back options if we encounter new words. So following our example the model has learned the following:\n",
    "\n",
    "```python\n",
    "[\n",
    "  ('I like learning', 'NLP'),\n",
    "  ('I like', 'learning'), ('like learning', 'NLP'),\n",
    "  ('I', 'like'), ('like', 'learning'), ('learning', 'NLP'),\n",
    "]\n",
    "```\n",
    "\n",
    "This is done so that if we encounter a sentence like `We like learning` we can fall back to the prefix of length 2 and then predict `NLP`.\n",
    "\n",
    "**Implement** the predict function that checks from the largest to the smaller possible prefix and uses the memory dictionary to make predictions and returns the top_k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4gLdAfbTLdv"
   },
   "source": [
    "## ASSIGNMENT PART 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cd9kXfokbZzn"
   },
   "source": [
    "def _window(seq, n=2):\n",
    "  \"\"\"Returns a sliding window based on n\n",
    "  \"\"\"\n",
    "  seq = tuple(seq)\n",
    "  if len(seq) < n: \n",
    "    return []\n",
    "  for i in range(0, len(seq) - n + 1):\n",
    "    yield seq[i:i+n]\n",
    "\n",
    "\n",
    "class Memorizer:\n",
    "  def __init__(self, train_dataset, largest_prefix=3): \n",
    "    self.largest_prefix = largest_prefix\n",
    "    self.memory = {}\n",
    "    # Build the dictionaries for each prefix length\n",
    "    for prefix_size in range(largest_prefix+1):\n",
    "      self.memory[prefix_size] = defaultdict(Counter)\n",
    "      self._build(train_dataset, prefix_size + 1, self.memory[prefix_size])\n",
    "\n",
    "  def _build(self, train_dataset, window_size, memory):\n",
    "    \"\"\"Build the memory dictionary for a provided window_size\n",
    "    \"\"\"\n",
    "    for data in train_dataset:\n",
    "      words = data.split(\" \")\n",
    "      # Compute the different word windows using the _window function\n",
    "      for window in _window(words, window_size):\n",
    "        if window_size == 1:\n",
    "          # There is no window, just memorize how frequently each word occurs in the dataset\n",
    "          output_word = window[0]\n",
    "          # Default all the prefixes to UNK\n",
    "          prefix = \"UNK\"\n",
    "        else:\n",
    "          # Use the prefix and update the count of the word that follows it\n",
    "          prefix = \" \".join(window[:-1])\n",
    "          output_word = window[-1]\n",
    "        memory[prefix][output_word] += 1\n",
    "\n",
    "  def predict(self, prefix, top_k=1):\n",
    "    \"\"\"Top_k words that might follow the given the prefix in our dataset\n",
    "    \"\"\"\n",
    "    prefix_words = prefix.split(\" \")\n",
    "    for prefix_len in range(min(len(prefix_words), self.largest_prefix), 0, -1):\n",
    "\n",
    "      # Compute the prefix string for the size of the window\n",
    "      ### TO BE COMPLETED ### \n",
    "      prefix_str = \" \".join(prefix_words[-prefix_len:])\n",
    "\n",
    "      # If prefix is in memory \"return\" the top_k matches \n",
    "      # Remember we've to return here since we want to use the data from the longest prefix that matches\n",
    "      if prefix_str in self.memory[prefix_len]:\n",
    "        ### TO BE COMPLETED ###\n",
    "        top_counters = self.memory[prefix_len][prefix_str].most_common(top_k)\n",
    "        top_tokens = [top_counter[0] for top_counter in top_counters]\n",
    "        return top_tokens\n",
    "\n",
    "    # None of the prefix matched so just return the most common words in the dataset\n",
    "    predictions = self.memory[0][\"UNK\"].most_common(top_k)\n",
    "    return [p[0] for p in predictions]"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VazPFsZ1Ti5p"
   },
   "source": [
    "## Experiment with Memorizer widget\n",
    "\n",
    "Ha! Here is a cute trick to build fun widgets within the colab. Just try different sentences for the dataset and prefix to see if the memorizer is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WOhihw5x8oRU"
   },
   "source": [
    "#@title Experiment with Memorizer\n",
    "\"\"\"\n",
    "In this cell, we've built a toy dataset with only 3 examples. \n",
    "Now given the prefix 'I like', a trie would emit 'football' and 'tennis' based \n",
    "on the co-occurence.\n",
    "\"\"\"\n",
    "dataset_1 = \"I like football\" #@param {type:\"string\"}\n",
    "dataset_2 = \"I like tennis sometimes\" #@param {type:\"string\"}\n",
    "dataset_3 = \"I like football way too much\" #@param {type:\"string\"}\n",
    "prefix = \"I like\" #@param {type:\"string\"}\n",
    "\n",
    "memorized_toy_model = Memorizer([dataset_1, dataset_2, dataset_3])\n",
    "\n",
    "predictions = memorized_toy_model.predict(prefix, 2)\n",
    "\n",
    "# The model should predict [football, tennis]\n",
    "# since football occurred twice while tennis was just once.\n",
    "print(\"Predictions: \", predictions)"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  ['football', 'tennis']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr0JHmxsT79f"
   },
   "source": [
    "### Train memorizer on the actual training data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v6V8N3peEbhd"
   },
   "source": [
    "memorized_model = Memorizer(train_dataset)"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-kJxjxUABN"
   },
   "source": [
    "### Evaluation on top_1 and top_3\n",
    "##### <font color='red'>Expected em@1: ~0.12%</font>\n",
    "##### <font color='red'>Expected em@3: ~0.122%</font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0azpnN_aF9Vl"
   },
   "source": [
    "evaluate_exact_match_at(memorized_model, prefixes, output_words, 1)"
   ],
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@1:0.178 . Model got 89 matches out of 500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ml6fiVhaCOhC"
   },
   "source": [
    "evaluate_exact_match_at(memorized_model, prefixes, output_words, 3)"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@3:0.246 . Model got 123 matches out of 500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omWKcpYxHRpT"
   },
   "source": [
    "## GPT-2: Generative Pre-trained Transformer\n",
    "\n",
    "We'll use the pretrainined GPT-2 model provided by the transformers package. Make sure you implement the predict function.\n",
    "\n",
    "Implementation Steps:\n",
    "1. Encode the sentence using `tokenizer.encode` and make sure it returns a torch tensor.\n",
    "2. Run this through the model and those are your predictions.\n",
    "3. Decode the indices from the output of top_k using the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1VOee59YGU1"
   },
   "source": [
    "### ASSIGNMENT PART 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B0WYO_MpXoDl"
   },
   "source": [
    "class GPT2PreTrained:\n",
    "  def __init__(self): \n",
    "    self.tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
    "    self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    self.model.eval()\n",
    "\n",
    "  def predict(self, prefix, top_k=1):\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "    indexed_tokens = self.tokenizer(prefix, return_tensors=\"pt\")\n",
    "    predictions = self.model(**indexed_tokens)\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "\n",
    "    _, indices = torch.topk(predictions[0][0, -1, :], k=top_k)\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "    predictions = [self.tokenizer.decode(id) for id in indices]\n",
    "    \n",
    "    return predictions"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foxA5kgTXrjN"
   },
   "source": [
    "### Experiment with GPT-2 Widget"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8MQXTUvPDp2k"
   },
   "source": [
    "#@title Experiment with GPT-2\n",
    "\"\"\"\n",
    "In this cell, we've built a toy prompt from which we predict \n",
    "the next words using GPT-2.\n",
    "\"\"\"\n",
    "text = \"pitcher threw a\" #@param {type:\"string\"}\n",
    "\n",
    "gpt_model = GPT2PreTrained()\n",
    "\n",
    "predictions = gpt_model.predict(text, 2)\n",
    "## Output should be \"pitch, ball\" or something similar\n",
    "print(\"Predictions: \", predictions)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [' pitch', ' ball']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y4sqtI2YA7M"
   },
   "source": [
    "### Evaluation on top_1 and top_3\n",
    "##### <font color='red'>Expected em@1: ~0.21%</font>\n",
    "##### <font color='red'>Expected em@3: ~0.298%</font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wb1gU6S5IyXW"
   },
   "source": [
    "evaluate_exact_match_at(gpt_model, prefixes, output_words, 1)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@1:0.21 . Model got 105 matches out of 500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "88s8LvLgA3le"
   },
   "source": [
    "evaluate_exact_match_at(gpt_model, prefixes, output_words, 3)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@3:0.324 . Model got 162 matches out of 500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLtHM8gyavwB"
   },
   "source": [
    "ðŸŽ‰ YAYYYY!!! We did it, that's it. Take a second to pause how many different things you've tried in the last 4 weeks. Go you!!\n",
    "\n",
    "## Extensions\n",
    "- Build an LSTM based generation model (Remember to cut sequences at about 10-15 words, LSTMs don't work on long sentences).\n",
    "- Try fine-tuning the GPT-2 model using a GPU runtime for the notebook. (NOTE: colab free GPUs are pretty bad so this is probably not worth doing in the free tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
