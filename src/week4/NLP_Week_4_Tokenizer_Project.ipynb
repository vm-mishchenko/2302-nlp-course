{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z0emNvIe_n5"
      },
      "source": [
        "# Week 4: Tokenization\n",
        "\n",
        "### What we are building\n",
        "Tokenization is the task of chopping it up into pieces, called tokens. As you might have observed going through the projects in Week 1, vectorization and tokenization have a huge influence on the output of the exact same model.\n",
        "\n",
        "We will compare the different tokenizers for different sizes of vocabulary on Botchan a novel written by Natsume Sōseki in 1906. We'll see what percentage of vocabulary would be considered OOV (out-of-vocab) at different sizes.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. We have provide scaffolding for all the different tokenizers and added an assert to make sure the output is same as expected.\n",
        "1. Most of the tokenizers are already somethings you've seen before so but we'll dive deeper into SentencePiece.\n",
        "\n",
        "### Code Overview\n",
        "\n",
        "- Dependencies: Install and import python dependencies\n",
        "- Tokenizers\n",
        "  - WhitespaceTokenizer\n",
        "  - CharacterTokenizer\n",
        "  - SpacyTokenizer\n",
        "  - BERTTokenizer\n",
        "  - SentencePieceTokenizer\n",
        "- Extensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8zLCEfd7VKI"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "✨ Now let's get started! To kick things off, as always, we will install some dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MmzNWDu3Sxy"
      },
      "source": [
        "%%capture\n",
        "# Install all the required dependencies for the project\n",
        "!pip install spacy --quiet\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTWZJAqiBxEv"
      },
      "source": [
        "Import all the necessary libraries we need throughout the project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Oy-47Jfkx4"
      },
      "source": [
        "# Import all the relevant libraries\n",
        "import re\n",
        "import en_core_web_md\n",
        "import sentencepiece as spm\n",
        "\n",
        "from collections import defaultdict\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWE7zx6Wnria"
      },
      "source": [
        "Now let's load the Spacy data, which comes with pre-trainined embeddings. This process is expensive so only do it once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QZcvlY53PK2"
      },
      "source": [
        "loaded_spacy_model = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFcN6rKkCQiu"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Download the Botchan novel from [SentencePiece repository](https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiSlP2uq3ywn"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
        "\n",
        "lines = []\n",
        "with open('botchan.txt', 'r') as the_file:\n",
        "  lines = the_file.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmb43XRQHr3I"
      },
      "source": [
        "Constants for sample sentence and thresholds we'll be using throughout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmhZpupceurg"
      },
      "source": [
        "SAMPLE_SENTENCE = \"I'm learning NLP. Aren't my projects awesome?\"\n",
        "THRESHOLDS = [1000, 2000, 3000, 4000, 5000, 7500, 10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IFTsT43Hzb9"
      },
      "source": [
        "### Base Tokenizer class that implements the coverage report function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TYoyB3cBQqX"
      },
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, lines):\n",
        "    self.vocab_size = 0\n",
        "    token_dict = defaultdict(int)\n",
        "\n",
        "    for line in lines:\n",
        "      for token in self.tokenize(line):\n",
        "        self.vocab_size += 1\n",
        "        token_dict[token] += 1\n",
        "\n",
        "    self.token_counts = sorted(token_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  def tokenize(self, sentence):\n",
        "    raise ValueError(\"TO BE IMPLEMENTED\")\n",
        "\n",
        "  def coverage(self, threshold):\n",
        "    return sum([x[1] for x in self.token_counts[:threshold]]) / self.vocab_size\n",
        "\n",
        "  def coverage_report(self, thresholds):\n",
        "    # For each threshold print the percentage of coverage and OOV\n",
        "    for tv in thresholds:\n",
        "      coverage = self.coverage(tv) * 100\n",
        "      print(\"For vocab size: %d, coverage is: %.2f%% and oov is: %.2f%%\" % (tv, coverage, 100-coverage))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MOIayXJfBMG"
      },
      "source": [
        "## Assignment Part:1 - Whitespace based separators\n",
        "##### <font color='red'>Expected vocab: 1.000, coverage: 72.79%</font>\n",
        "##### <font color='red'>Expected vocab: 10.000, coverage: 99.43%</font>\n",
        "\n",
        "Tokenizer that splits the string sentences into tokens using  whitespace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R__gkTBMH7fD"
      },
      "source": [
        "class WhiteSpaceTokenizer(Tokenizer):\n",
        "  def tokenize(self, sentence):\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    output = ...\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    \n",
        "    return output\n",
        "\n",
        "white_space_tokenizer = WhiteSpaceTokenizer(lines)\n",
        "assert white_space_tokenizer.tokenize(SAMPLE_SENTENCE) == [\"I'm\", 'learning', 'NLP.', \"Aren't\", 'my', 'projects', 'awesome?']\n",
        "white_space_tokenizer.coverage_report(THRESHOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6RKpXenfS2i"
      },
      "source": [
        "## Assignment Part:2 - Character based tokenizer\n",
        "##### <font color='red'>Expected vocab: 1.000, coverage: 100%</font>\n",
        "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
        "\n",
        "Tokenizer that splits the string sentences into individual characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5gy-81wIb9i"
      },
      "source": [
        "class CharacterTokenizer(Tokenizer):\n",
        "  def tokenize(self, sentence):\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    output = ...\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    \n",
        "    return output\n",
        "\n",
        "character_tokenizer = CharacterTokenizer(lines)\n",
        "assert character_tokenizer.tokenize(SAMPLE_SENTENCE) == ['I', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'N', 'L', 'P', '.', ' ', 'A', 'r', 'e', 'n', \"'\", 't', ' ', 'm', 'y', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '?']\n",
        "character_tokenizer.coverage_report(THRESHOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9cpffoCOa7"
      },
      "source": [
        "## Assignment Part:3 - Spacy Tokenizer\n",
        "##### <font color='red'>Expected vocab: 1.000, coverage: 86%</font>\n",
        "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
        "\n",
        "Tokenizer that splits the string sentences into individual tokens using the Spacy's built in tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glf0u_PNCScI"
      },
      "source": [
        "class SpacyTokenizer(Tokenizer):\n",
        "  def tokenize(self, sentence):\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    output = ...\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    \n",
        "    return output\n",
        "\n",
        "spacy_tokenizer = SpacyTokenizer(lines)\n",
        "assert spacy_tokenizer.tokenize(SAMPLE_SENTENCE) == ['I', \"'m\", 'learning', 'NLP', '.', 'Are', \"n't\", 'my', 'projects', 'awesome', '?']\n",
        "spacy_tokenizer.coverage_report(THRESHOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5OU-oomCs-0"
      },
      "source": [
        "## Assignment Part:4 - BERT Tokenizer\n",
        "##### <font color='red'>Expected vocab: 1.000, coverage: 85.46%</font>\n",
        "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
        "\n",
        "BERT tokenizer provided by the hugging face library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYNEK_eNCvTJ"
      },
      "source": [
        "class BERTTokenizer(Tokenizer):\n",
        "  def __init__(self, tokenizer, lines):\n",
        "    self.tokenizer = tokenizer\n",
        "    super(BERTTokenizer, self).__init__(lines)\n",
        "\n",
        "  def tokenize(self, sentence):\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    output = ...\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    \n",
        "    return output\n",
        "\n",
        "raw_bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_tokenizer = BERTTokenizer(raw_bert_tokenizer, lines)\n",
        "assert bert_tokenizer.tokenize(SAMPLE_SENTENCE) == ['i', \"'\", 'm', 'learning', 'nl', '##p', '.', 'aren', \"'\", 't', 'my', 'projects', 'awesome', '?'], bert_tokenizer.tokenize(SAMPLE_SENTENCE)\n",
        "bert_tokenizer.coverage_report(THRESHOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdls0YKBFL-F"
      },
      "source": [
        "## Assignment Part:5 - SentencePieceTokenizer\n",
        "##### <font color='red'>Expected vocab: 1.000, coverage: 99.87%</font>\n",
        "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
        "\n",
        "SentencePiece tokenizer works a bit differently from everything we've used so far. It needs to be trained with a vocabulary and a target size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBcbz02pFPf4"
      },
      "source": [
        "class SentencePieceTokenizer(Tokenizer):\n",
        "  def __init__(self, lines):\n",
        "    self.lines = lines\n",
        "    spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=4000)\n",
        "    self.sp = spm.SentencePieceProcessor()\n",
        "    self.sp.load('m.model')\n",
        "\n",
        "  def tokenize(self, sentence):\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    output = ...\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    \n",
        "    return output\n",
        "\n",
        "  def coverage(self, threshold):\n",
        "    # Train a new SentencePiece tokenizer for the threshold provided\n",
        "    try:\n",
        "      spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix=f'm{threshold}', vocab_size=threshold)\n",
        "    except RuntimeError:\n",
        "      # Vocabulary size > 5239 raises a runtime error\n",
        "      return 1.0 \n",
        "\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    # Load our recently trained model\n",
        "    sp.load(f'm{threshold}.model')\n",
        "\n",
        "    # Count the number of times UNK (id=0) was assigned in the entire dataset from the model\n",
        "    total = 0\n",
        "    unk = 0\n",
        "    for line in self.lines:\n",
        "      ids = sp.encode_as_ids(line)\n",
        "      unk += ids.count(0)\n",
        "      total += len(ids)\n",
        "    return (total - unk) / total\n",
        "\n",
        "sp_tokenizer = SentencePieceTokenizer(lines)\n",
        "assert sp_tokenizer.tokenize(SAMPLE_SENTENCE) == ['▁I', \"'\", 'm', '▁learn', 'ing', '▁N', 'L', 'P', '.', '▁A', 'ren', \"'\", 't', '▁my', '▁pro', 'j', 'e', 'c', 't', 's', '▁awe', 'some', '?'], sp_tokenizer.tokenize(SAMPLE_SENTENCE)\n",
        "sp_tokenizer.coverage_report(THRESHOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksluayoLPIq"
      },
      "source": [
        "As you see the OOV% is really low but at the same time it increased a bit when \n",
        "\n",
        "---\n",
        "\n",
        "we increased a vocabulary size. An intuitive way to think about this is that in smaller vocabularies the algorithm trains to something closer to character embeddings as we give it a bit larger size it tries to learn more language semantics and trades off some vocabulary coverage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e18CnCtdMgIS"
      },
      "source": [
        "🎉 WOOHOOO we've covered the part 1 of this week. Let's keep making progress and proceed to the Generation notebook. But do come back to try out some of the extensions.\n",
        "\n",
        "# Extensions\n",
        "\n",
        "Now that you've worked through the part 1 of the project there is a lot more for us to try:\n",
        "\n",
        "- Try the new tokenizers in the Week 1 EmbeddingBag model?\n",
        "- Similarly change the tokenizer in the Week 2 LSTM?\n",
        "- Compare the tokenizers on a non-english language data?"
      ]
    }
  ]
}