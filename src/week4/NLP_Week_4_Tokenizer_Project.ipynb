{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z0emNvIe_n5"
   },
   "source": [
    "# Week 4: Tokenization\n",
    "\n",
    "### What we are building\n",
    "Tokenization is the task of chopping it up into pieces, called tokens. As you might have observed going through the projects in Week 1, vectorization and tokenization have a huge influence on the output of the exact same model.\n",
    "\n",
    "We will compare the different tokenizers for different sizes of vocabulary on Botchan a novel written by Natsume Sōseki in 1906. We'll see what percentage of vocabulary would be considered OOV (out-of-vocab) at different sizes.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We have provide scaffolding for all the different tokenizers and added an assert to make sure the output is same as expected.\n",
    "1. Most of the tokenizers are already somethings you've seen before so but we'll dive deeper into SentencePiece.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Tokenizers\n",
    "  - WhitespaceTokenizer\n",
    "  - CharacterTokenizer\n",
    "  - SpacyTokenizer\n",
    "  - BERTTokenizer\n",
    "  - SentencePieceTokenizer\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8zLCEfd7VKI"
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "✨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1MmzNWDu3Sxy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting en_core_web_md==2.2.5\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\r\n",
      "     |████████████████████████████████| 96.4 MB 20.1 MB/s            \r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: spacy>=2.2.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from en_core_web_md==2.2.5) (2.2.4)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.9)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.8)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.10.1)\r\n",
      "Requirement already satisfied: setuptools in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (60.2.0)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.64.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.28.2)\r\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.6)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.13.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.5.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_md')\r\n",
      "Requirement already satisfied: transformers in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (4.17.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (0.13.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (6.0.0)\r\n",
      "Requirement already satisfied: filelock in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (3.9.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (2.28.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (23.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (2022.10.31)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (4.64.1)\r\n",
      "Requirement already satisfied: sacremoses in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (0.0.53)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers) (1.21.6)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.5.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.13.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers) (3.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: click in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: six in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: sentencepiece in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (0.1.97)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install all the required dependencies for the project\n",
    "!pip install spacy --quiet\n",
    "!python -m spacy download en_core_web_md\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTWZJAqiBxEv"
   },
   "source": [
    "Import all the necessary libraries we need throughout the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "41Oy-47Jfkx4"
   },
   "outputs": [],
   "source": [
    "# Import all the relevant libraries\n",
    "import re\n",
    "import en_core_web_md\n",
    "import sentencepiece as spm\n",
    "\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWE7zx6Wnria"
   },
   "source": [
    "Now let's load the Spacy data, which comes with pre-trainined embeddings. This process is expensive so only do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8QZcvlY53PK2"
   },
   "outputs": [],
   "source": [
    "loaded_spacy_model = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFcN6rKkCQiu"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Download the Botchan novel from [SentencePiece repository](https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OiSlP2uq3ywn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-08 11:37:35--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 278779 (272K) [text/plain]\r\n",
      "Saving to: ‘botchan.txt’\r\n",
      "\r\n",
      "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.07s   \r\n",
      "\r\n",
      "2023-03-08 11:37:36 (3.81 MB/s) - ‘botchan.txt’ saved [278779/278779]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
    "\n",
    "lines = []\n",
    "with open('botchan.txt', 'r') as the_file:\n",
    "  lines = the_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmb43XRQHr3I"
   },
   "source": [
    "Constants for sample sentence and thresholds we'll be using throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rmhZpupceurg"
   },
   "outputs": [],
   "source": [
    "SAMPLE_SENTENCE = \"I'm learning NLP. Aren't my projects awesome?\"\n",
    "THRESHOLDS = [1000, 2000, 3000, 4000, 5000, 7500, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IFTsT43Hzb9"
   },
   "source": [
    "### Base Tokenizer class that implements the coverage report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-TYoyB3cBQqX"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  def __init__(self, lines):\n",
    "    self.vocab_size = 0\n",
    "    token_dict = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "      for token in self.tokenize(line):\n",
    "        self.vocab_size += 1\n",
    "        token_dict[token] += 1\n",
    "\n",
    "    self.token_counts = sorted(token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  def tokenize(self, sentence):\n",
    "    raise ValueError(\"TO BE IMPLEMENTED\")\n",
    "\n",
    "  def coverage(self, threshold):\n",
    "    return sum([x[1] for x in self.token_counts[:threshold]]) / self.vocab_size\n",
    "\n",
    "  def coverage_report(self, thresholds):\n",
    "    # For each threshold print the percentage of coverage and OOV\n",
    "    for tv in thresholds:\n",
    "      coverage = self.coverage(tv) * 100\n",
    "      print(\"For vocab size: %d, coverage is: %.2f%% and oov is: %.2f%%\" % (tv, coverage, 100-coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MOIayXJfBMG"
   },
   "source": [
    "## Assignment Part:1 - Whitespace based separators\n",
    "##### <font color='red'>Expected vocab: 1.000, coverage: 72.79%</font>\n",
    "##### <font color='red'>Expected vocab: 10.000, coverage: 99.43%</font>\n",
    "\n",
    "Tokenizer that splits the string sentences into tokens using  whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R__gkTBMH7fD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 75.34% and oov is: 24.66%\n",
      "For vocab size: 2000, coverage is: 82.58% and oov is: 17.42%\n",
      "For vocab size: 3000, coverage is: 86.77% and oov is: 13.23%\n",
      "For vocab size: 4000, coverage is: 89.78% and oov is: 10.22%\n",
      "For vocab size: 5000, coverage is: 91.75% and oov is: 8.25%\n",
      "For vocab size: 7500, coverage is: 96.68% and oov is: 3.32%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class WhiteSpaceTokenizer(Tokenizer):\n",
    "  def tokenize(self, sentence):\n",
    "    ### + TO BE IMPLEMENTED ###\n",
    "    output = sentence.split()\n",
    "    \n",
    "    return output\n",
    "\n",
    "white_space_tokenizer = WhiteSpaceTokenizer(lines)\n",
    "assert white_space_tokenizer.tokenize(SAMPLE_SENTENCE) == [\"I'm\", 'learning', 'NLP.', \"Aren't\", 'my', 'projects', 'awesome?']\n",
    "white_space_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6RKpXenfS2i"
   },
   "source": [
    "## Assignment Part:2 - Character based tokenizer\n",
    "##### <font color='red'>Expected vocab: 1.000, coverage: 100%</font>\n",
    "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
    "\n",
    "Tokenizer that splits the string sentences into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "i5gy-81wIb9i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 2000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 3000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 4000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 5000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class CharacterTokenizer(Tokenizer):\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = [*sentence]\n",
    "    \n",
    "    return output\n",
    "\n",
    "character_tokenizer = CharacterTokenizer(lines)\n",
    "assert character_tokenizer.tokenize(SAMPLE_SENTENCE) == ['I', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'N', 'L', 'P', '.', ' ', 'A', 'r', 'e', 'n', \"'\", 't', ' ', 'm', 'y', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '?']\n",
    "character_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU9cpffoCOa7"
   },
   "source": [
    "## Assignment Part:3 - Spacy Tokenizer\n",
    "##### <font color='red'>Expected vocab: 1.000, coverage: 86%</font>\n",
    "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
    "\n",
    "Tokenizer that splits the string sentences into individual tokens using the Spacy's built in tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Glf0u_PNCScI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 86.01% and oov is: 13.99%\n",
      "For vocab size: 2000, coverage is: 91.93% and oov is: 8.07%\n",
      "For vocab size: 3000, coverage is: 95.08% and oov is: 4.92%\n",
      "For vocab size: 4000, coverage is: 96.67% and oov is: 3.33%\n",
      "For vocab size: 5000, coverage is: 98.21% and oov is: 1.79%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class SpacyTokenizer(Tokenizer):\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    doc = loaded_spacy_model(sentence)\n",
    "    output = [token.text for token in doc]\n",
    "    \n",
    "    return output\n",
    "\n",
    "spacy_tokenizer = SpacyTokenizer(lines)\n",
    "assert spacy_tokenizer.tokenize(SAMPLE_SENTENCE) == ['I', \"'m\", 'learning', 'NLP', '.', 'Are', \"n't\", 'my', 'projects', 'awesome', '?']\n",
    "spacy_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5OU-oomCs-0"
   },
   "source": [
    "## Assignment Part:4 - BERT Tokenizer\n",
    "##### <font color='red'>Expected vocab: 1.000, coverage: 85.46%</font>\n",
    "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
    "\n",
    "BERT tokenizer provided by the hugging face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PYNEK_eNCvTJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 85.46% and oov is: 14.54%\n",
      "For vocab size: 2000, coverage is: 92.11% and oov is: 7.89%\n",
      "For vocab size: 3000, coverage is: 95.61% and oov is: 4.39%\n",
      "For vocab size: 4000, coverage is: 97.51% and oov is: 2.49%\n",
      "For vocab size: 5000, coverage is: 99.04% and oov is: 0.96%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class BERTTokenizer(Tokenizer):\n",
    "  def __init__(self, tokenizer, lines):\n",
    "    self.tokenizer = tokenizer\n",
    "    super(BERTTokenizer, self).__init__(lines)\n",
    "\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    tokens = self.tokenizer(\n",
    "      sentence\n",
    "    )\n",
    "\n",
    "    output = self.tokenizer.convert_ids_to_tokens(tokens.input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return output\n",
    "\n",
    "raw_bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BERTTokenizer(raw_bert_tokenizer, lines)\n",
    "assert bert_tokenizer.tokenize(SAMPLE_SENTENCE) == ['i', \"'\", 'm', 'learning', 'nl', '##p', '.', 'aren', \"'\", 't', 'my', 'projects', 'awesome', '?'], bert_tokenizer.tokenize(SAMPLE_SENTENCE)\n",
    "bert_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdls0YKBFL-F"
   },
   "source": [
    "## Assignment Part:5 - SentencePieceTokenizer\n",
    "##### <font color='red'>Expected vocab: 1.000, coverage: 99.87%</font>\n",
    "##### <font color='red'>Expected vocab: 10.000, coverage: 100%</font>\n",
    "\n",
    "SentencePiece tokenizer works a bit differently from everything we've used so far. It needs to be trained with a vocabulary and a target size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YBcbz02pFPf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4396 obj=8.63568 num_tokens=19702 num_tokens/piece=4.4818\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4389 obj=8.60711 num_tokens=19706 num_tokens/piece=4.48986\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m1000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2941 obj=8.95617 num_tokens=22741 num_tokens/piece=7.7324\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2941 obj=8.88103 num_tokens=22745 num_tokens/piece=7.73376\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2205 obj=9.26224 num_tokens=25461 num_tokens/piece=11.5469\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2205 obj=9.17719 num_tokens=25457 num_tokens/piece=11.5451\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1653 obj=9.63804 num_tokens=28626 num_tokens/piece=17.3176\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1653 obj=9.55479 num_tokens=28629 num_tokens/piece=17.3194\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1239 obj=10.1048 num_tokens=31904 num_tokens/piece=25.7498\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1239 obj=10.0166 num_tokens=31912 num_tokens/piece=25.7563\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1100 obj=10.2335 num_tokens=33166 num_tokens/piece=30.1509\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1100 obj=10.2025 num_tokens=33166 num_tokens/piece=30.1509\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m1000.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m1000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 99.87% and oov is: 0.13%\n",
      "For vocab size: 2000, coverage is: 99.85% and oov is: 0.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m2000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2941 obj=8.95617 num_tokens=22741 num_tokens/piece=7.7324\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2941 obj=8.88103 num_tokens=22745 num_tokens/piece=7.73376\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2205 obj=9.26224 num_tokens=25461 num_tokens/piece=11.5469\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2205 obj=9.17719 num_tokens=25457 num_tokens/piece=11.5451\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2200 obj=9.17892 num_tokens=25475 num_tokens/piece=11.5795\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2200 obj=9.17823 num_tokens=25475 num_tokens/piece=11.5795\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m2000.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m2000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m3000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3300 obj=8.80564 num_tokens=21670 num_tokens/piece=6.56667\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3300 obj=8.7678 num_tokens=21671 num_tokens/piece=6.56697\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m3000.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m3000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m4000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4396 obj=8.63568 num_tokens=19702 num_tokens/piece=4.4818\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4389 obj=8.60711 num_tokens=19706 num_tokens/piece=4.48986\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m4000.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m4000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 3000, coverage is: 99.84% and oov is: 0.16%\n",
      "For vocab size: 4000, coverage is: 99.83% and oov is: 0.17%\n",
      "For vocab size: 5000, coverage is: 99.83% and oov is: 0.17%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m5000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m5000.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m5000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m7500\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 7500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m7500.model\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m10000.model\n"
     ]
    }
   ],
   "source": [
    "class SentencePieceTokenizer(Tokenizer):\n",
    "  def __init__(self, lines):\n",
    "    self.lines = lines\n",
    "    spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=4000)\n",
    "    self.sp = spm.SentencePieceProcessor()\n",
    "    self.sp.load('m.model')\n",
    "\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = self.sp.encode_as_pieces(sentence)\n",
    "    \n",
    "    return output\n",
    "\n",
    "  def coverage(self, threshold):\n",
    "    # Train a new SentencePiece tokenizer for the threshold provided\n",
    "    try:\n",
    "      spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix=f'm{threshold}', vocab_size=threshold)\n",
    "    except RuntimeError:\n",
    "      # Vocabulary size > 5239 raises a runtime error\n",
    "      return 1.0 \n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    # Load our recently trained model\n",
    "    sp.load(f'm{threshold}.model')\n",
    "\n",
    "    # Count the number of times UNK (id=0) was assigned in the entire dataset from the model\n",
    "    total = 0\n",
    "    unk = 0\n",
    "    for line in self.lines:\n",
    "      ids = sp.encode_as_ids(line)\n",
    "      unk += ids.count(0)\n",
    "      total += len(ids)\n",
    "    return (total - unk) / total\n",
    "\n",
    "sp_tokenizer = SentencePieceTokenizer(lines)\n",
    "assert sp_tokenizer.tokenize(SAMPLE_SENTENCE) == ['▁I', \"'\", 'm', '▁learn', 'ing', '▁N', 'L', 'P', '.', '▁A', 'ren', \"'\", 't', '▁my', '▁pro', 'j', 'e', 'c', 't', 's', '▁awe', 'some', '?'], sp_tokenizer.tokenize(SAMPLE_SENTENCE)\n",
    "sp_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ksluayoLPIq"
   },
   "source": [
    "As you see the OOV% is really low but at the same time it increased a bit when \n",
    "\n",
    "---\n",
    "\n",
    "we increased a vocabulary size. An intuitive way to think about this is that in smaller vocabularies the algorithm trains to something closer to character embeddings as we give it a bit larger size it tries to learn more language semantics and trades off some vocabulary coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e18CnCtdMgIS"
   },
   "source": [
    "🎉 WOOHOOO we've covered the part 1 of this week. Let's keep making progress and proceed to the Generation notebook. But do come back to try out some of the extensions.\n",
    "\n",
    "# Extensions\n",
    "\n",
    "Now that you've worked through the part 1 of the project there is a lot more for us to try:\n",
    "\n",
    "- Try the new tokenizers in the Week 1 EmbeddingBag model?\n",
    "- Similarly change the tokenizer in the Week 2 LSTM?\n",
    "- Compare the tokenizers on a non-english language data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
