{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning==1.6.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.6.5)\r\n",
      "Requirement already satisfied: spacy==2.2.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (2.2.4)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (0.11.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.5.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.21.6)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (0.3.2)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (3.20.1)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (2023.1.0)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.64.1)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (2.11.2)\r\n",
      "Requirement already satisfied: torch>=1.8.* in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.13.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (6.0)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (23.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.9)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (3.0.8)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.1.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (60.2.0)\r\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (7.4.0)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (0.4.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (2.0.7)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (2.28.2)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.2)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (0.10.1)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.6)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.13.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.8.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.12.7)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.16.1)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.51.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.4.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.6)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.2.3)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.37.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.4.1)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (0.13.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (22.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (4.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.8.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.3)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.16.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (5.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (4.9)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (6.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.1.2)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.2.2)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: pandas in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas) (1.21.6)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas) (2022.7.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning==1.6.5 spacy==2.2.4\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from collections import Counter\n",
    "import en_core_web_md\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "loaded_spacy_model = en_core_web_md.load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Fix the random seed so that we get consistent results\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-03 13:38:23--  https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 28022709 (27M) [application/gzip]\r\n",
      "Saving to: ‘empatheticdialogues.tar.gz’\r\n",
      "\r\n",
      "empatheticdialogues 100%[===================>]  26.72M  16.0MB/s    in 1.7s    \r\n",
      "\r\n",
      "2023-03-03 13:38:26 (16.0 MB/s) - ‘empatheticdialogues.tar.gz’ saved [28022709/28022709]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import csv\n",
    "\n",
    "DIRECTORY_NAME=\"classification\"\n",
    "TRAIN_FILE=\"classification/empatheticdialogues/train.csv\"\n",
    "VALIDATION_FILE=\"classification/empatheticdialogues/valid.csv\"\n",
    "TEST_FILE=\"classification/empatheticdialogues/test.csv\"\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "  \"\"\"\n",
    "  Download the dialog dataset. The tarball contains three files: train.csv, valid.csv, test.csv\n",
    "  \"\"\"\n",
    "\n",
    "  # if running locally, install wget before by running: \"brew install wget\"\n",
    "  # \"!pip install wget\" won't help as it installs python module not cli\n",
    "  !wget 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "  # !wget help\n",
    "  if not os.path.isdir(DIRECTORY_NAME):\n",
    "    !mkdir classification\n",
    "  tar = tarfile.open('empatheticdialogues.tar.gz')\n",
    "  tar.extractall(DIRECTORY_NAME)\n",
    "  tar.close()\n",
    "\n",
    "# Expensive operation so we should just do this once\n",
    "download_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['classification/empatheticdialogues/valid.csv',\n 'classification/empatheticdialogues/test.csv',\n 'classification/empatheticdialogues/train.csv']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "['classification/empatheticdialogues/valid.csv',\n 'classification/empatheticdialogues/test.csv',\n 'classification/empatheticdialogues/train.csv']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "['classification/empatheticdialogues/valid.csv',\n 'classification/empatheticdialogues/test.csv',\n 'classification/empatheticdialogues/train.csv']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(f\"{DIRECTORY_NAME}/**/*.csv\", recursive=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0       conv_id utterance_idx      context  \\\n1  hit:0_conv:1             1  sentimental   \n2  hit:0_conv:1             2  sentimental   \n3  hit:0_conv:1             3  sentimental   \n4  hit:0_conv:1             4  sentimental   \n5  hit:0_conv:1             5  sentimental   \n\n0                                             prompt speaker_idx  \\\n1  I remember going to the fireworks with my best...           1   \n2  I remember going to the fireworks with my best...           0   \n3  I remember going to the fireworks with my best...           1   \n4  I remember going to the fireworks with my best...           0   \n5  I remember going to the fireworks with my best...           1   \n\n0                                          utterance     selfeval tags  \n1  I remember going to see the fireworks with my ...  5|5|5_2|2|5       \n2  Was this a friend you were in love with_comma_...  5|5|5_2|2|5       \n3                This was a best friend. I miss her.  5|5|5_2|2|5       \n4                                Where has she gone?  5|5|5_2|2|5       \n5                                 We no longer talk.  5|5|5_2|2|5       ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conv_id</th>\n      <th>utterance_idx</th>\n      <th>context</th>\n      <th>prompt</th>\n      <th>speaker_idx</th>\n      <th>utterance</th>\n      <th>selfeval</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>hit:0_conv:1</td>\n      <td>1</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>1</td>\n      <td>I remember going to see the fireworks with my ...</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hit:0_conv:1</td>\n      <td>2</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>0</td>\n      <td>Was this a friend you were in love with_comma_...</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hit:0_conv:1</td>\n      <td>3</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>1</td>\n      <td>This was a best friend. I miss her.</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hit:0_conv:1</td>\n      <td>4</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>0</td>\n      <td>Where has she gone?</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>hit:0_conv:1</td>\n      <td>5</td>\n      <td>sentimental</td>\n      <td>I remember going to the fireworks with my best...</td>\n      <td>1</td>\n      <td>We no longer talk.</td>\n      <td>5|5|5_2|2|5</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the parse_dataset function below for short explanation.\n",
    "df = pd.read_csv(TRAIN_FILE, sep='\\n', header=None)\n",
    "df = df[0].str.split(',', expand=True)\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.columns = new_header\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# \"sentimental\" to \"<id>\"\n",
    "label_to_integer = dict()\n",
    "# \"<id>\" to \"sentimental\"\n",
    "integer_to_label = dict()\n",
    "\n",
    "for ix, label in enumerate(df[\"context\"].unique()):\n",
    "  label_to_integer[label] = ix\n",
    "  integer_to_label[ix] = label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def parse_dataset(file_path, sample=5000):\n",
    "  # Our dataset file is a csv with varying input lengths, therefore we load the\n",
    "  # file at once, we have to split it up into separate steps:\n",
    "  # 1. Read each row as a single column row\n",
    "  df = pd.read_csv(file_path, sep = '\\n', header = None)\n",
    "  # 2. Split up each row into separate columns\n",
    "  df = df[0].str.split(',', expand = True)\n",
    "  # 3. Set the header by using the first row\n",
    "  new_header = df.iloc[0]\n",
    "  df = df[1:]\n",
    "  df.columns = new_header\n",
    "\n",
    "  # Machine learning cannot work with categorical labels like \"surprised\" or\n",
    "  # \"excited\". Therefore, we convert these tokens into a number.\n",
    "  df[\"target\"] = df[\"context\"].apply(lambda x: label_to_integer[x])\n",
    "  df[\"feature\"] = df[\"prompt\"] + \" \" + df[\"utterance\"]\n",
    "\n",
    "  # We only need the column \"**feature**\" created from column\n",
    "  # \"**prompt**\" + \"**utterance**\" and the column \"**target**\".\n",
    "  return df[[\"target\", \"feature\"]].sample(n = sample, random_state = 0).values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: (40000, 2)\n",
      "Shape of validation dataset: (4000, 2)\n",
      "Shape of test dataset: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = parse_dataset(TRAIN_FILE, sample = 40000)\n",
    "validation_data = parse_dataset(VALIDATION_FILE, sample = 4000)\n",
    "test_data = parse_dataset(TEST_FILE, sample = 4000)\n",
    "\n",
    "print('Shape of training dataset: ({rows}, {cols})'.format(rows=len(training_data), cols=len(training_data[0])))\n",
    "print('Shape of validation dataset: ({rows}, {cols})'.format(rows=len(validation_data), cols=len(validation_data[0])))\n",
    "print('Shape of test dataset: ({rows}, {cols})'.format(rows=len(test_data), cols=len(test_data[0])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "  \"\"\"Creates an pytorch dataset to consume our pre-loaded csv data\n",
    "\n",
    "  Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "  \"\"\"\n",
    "  def __init__(self, data, vectorizer):\n",
    "    self.dataset = data\n",
    "    # Vectorizer needs to implement a vectorize function that returns vector and tokens\n",
    "    # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "    self.vectorizer = vectorizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    (label, sentence) = self.dataset[idx]\n",
    "    sentence_vector, sentence_tokens = self.vectorizer.vectorize(sentence)\n",
    "    return {\n",
    "      \"vectors\": sentence_vector,\n",
    "      \"label\": label,\n",
    "      \"tokens\": sentence_tokens, # for debugging only\n",
    "      \"sentence\": sentence # for debugging only\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class ClassificationDataModule(pl.LightningDataModule):\n",
    "  \"\"\"LightningDataModule: Wrapper class for the dataset to be used in training\n",
    "  \"\"\"\n",
    "  def __init__(self, vectorizer, params):\n",
    "    super().__init__()\n",
    "    self.params = params\n",
    "    self.classification_train = ClassificationDataset(training_data, vectorizer)\n",
    "    self.classification_val = ClassificationDataset(validation_data, vectorizer)\n",
    "    self.classification_test = ClassificationDataset(test_data, vectorizer)\n",
    "\n",
    "  # Function to convert the input raw data from the dataset into model input.\n",
    "  # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "  def collate_fn(self, batch):\n",
    "    # Embedding layers need the inputs to be integer, so we need to add this special case here.\n",
    "    if self.params.integer_input:\n",
    "      word_vector = [torch.LongTensor(item[\"vectors\"]) for item in batch]\n",
    "      sentence_vector = pad_sequence(word_vector, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "      sentence_vector = torch.stack([torch.Tensor(item[\"vectors\"]) for item in batch])\n",
    "    labels = torch.LongTensor([item[\"label\"] for item in batch])\n",
    "    return {\"vectors\": sentence_vector, \"labels\": labels, \"sentences\": [item[\"sentence\"] for item in batch]}\n",
    "\n",
    "  # Training dataloader .. will reset itself each epoch\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.classification_train, batch_size=self.params.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "  # Validation dataloader .. will reset itself each epoch\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.classification_val, batch_size=self.params.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "  # Test dataloader .. will reset itself each epoch\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.classification_test, batch_size=self.params.batch_size, collate_fn=self.collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class EmotionClassifier(pl.LightningModule):\n",
    "  def __init__(self, model, params):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.params = params\n",
    "    self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=params.num_classes)\n",
    "\n",
    "  # will be called automatically by \"training_step\", \"validation_step\", etc.\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"labels\"]\n",
    "    y_hat = self(x)\n",
    "    loss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    self.log_dict(\n",
    "      {'train_loss': loss},\n",
    "      batch_size=self.params.batch_size,\n",
    "      prog_bar=True\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_nb):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"labels\"]\n",
    "    y_hat = self(x)\n",
    "    val_loss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "      {\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': self.accuracy(predictions, y)\n",
    "      },\n",
    "      batch_size=self.params.batch_size,\n",
    "      prog_bar=True\n",
    "    )\n",
    "    return val_loss\n",
    "\n",
    "  def test_step(self, batch, batch_nb):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"labels\"]\n",
    "    y_hat = self(x)\n",
    "    test_loss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "      {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': self.accuracy(predictions, y)\n",
    "      },\n",
    "      batch_size=self.params.batch_size,\n",
    "      prog_bar=True\n",
    "    )\n",
    "    return test_loss\n",
    "\n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[\"vectors\"])\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    return {'logits':y_hat, 'predictions': predictions, 'labels': batch[\"labels\"], 'sentences': batch['sentences']}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.params.learning_rate)\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def trainer(model, params, vectorizer):\n",
    "  # Create a pytorch trainer\n",
    "  trainer = pl.Trainer(max_epochs=params.max_epochs, check_val_every_n_epoch=1)\n",
    "\n",
    "  # Initialize our data loader with the passed vectorizer\n",
    "  data_module = ClassificationDataModule(vectorizer, params)\n",
    "\n",
    "  # Instantiate a new model\n",
    "  emotionClassifier = EmotionClassifier(model, params)\n",
    "\n",
    "  # Train and validate the model\n",
    "  trainer.fit(emotionClassifier, data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader())\n",
    "\n",
    "  # Test the model\n",
    "  trainer.test(emotionClassifier, data_module.test_dataloader())\n",
    "\n",
    "  # Predict on the same test set to show some output\n",
    "  output = trainer.predict(emotionClassifier, data_module.test_dataloader())\n",
    "\n",
    "  for i in range(2):\n",
    "    print(\"-----------\")\n",
    "    print(\"Sentence: \", output[1]['sentences'][i])\n",
    "    print(\"Predicted Emotion: \", integer_to_label[output[1]['predictions'][i].item()])\n",
    "    print(\"Actual Label: \", integer_to_label[output[1]['labels'][i].item()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class WordVectorClassificationModel(torch.nn.Module):\n",
    "  def __init__(self, word_vec_dimension, num_classes):\n",
    "    super().__init__()\n",
    "    self.classes = num_classes\n",
    "    self.linear_layer = torch.nn.Linear(word_vec_dimension, num_classes)\n",
    "\n",
    "  # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "  def forward(self, batch):\n",
    "    \"\"\"Projection from word_vec_dim to n_classes\n",
    "\n",
    "    Batch is of shape (batch_size, max_seq_len, word_vector_dim)\n",
    "    \"\"\"\n",
    "    return self.linear_layer(batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class SpacyVectorizer:\n",
    "  def vectorize(self, sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, tokenize it and reference pre-trained word vector for each token.\n",
    "\n",
    "    Returns a tuple of sentence_vector and list of text tokens\n",
    "    \"\"\"\n",
    "    sentence_vector = []\n",
    "    sentence_tokens = []\n",
    "    # https://spacy.io/api/language#attributes\n",
    "    spacy_doc = loaded_spacy_model.make_doc(sentence) ## I am Sourabh\n",
    "    word_vector = [token.vector for token in spacy_doc] ## [ [Embedding of I], [Embedding of am], [Embedding of UNK]]\n",
    "    sentence_tokens = list([token.text for token in spacy_doc]) # [[I], [am], [Sourabh]]\n",
    "    sentence_vector = np.mean(np.array(word_vector), axis=0)\n",
    "    return sentence_vector, sentence_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/src/week3/lightning_logs\n",
      "\n",
      "  | Name     | Type                          | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model    | WordVectorClassificationModel | 9.6 K \n",
      "1 | accuracy | MulticlassAccuracy            | 0     \n",
      "-----------------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5534df69eb924467b6f205ade33515d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d5b28fc089748f8a85d12ed2c35ed15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f77cf9afc94d42c385da47e306a8bc78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb83e85d1e0c477bbd39b529decdcbd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3406181c77e4f3aa50e143403594b67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c88428bc6ad4fffbc103d430c4b9304"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37d6b13c9c2d4b998402b64e00f50e4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.3267500102519989\n",
      "        test_loss            2.456451892852783\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 1250it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cd37a5a33cc4f80b98b9d004142fef6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Sentence:  When I got falsely accused of eating my roommate's ice cream last night_comma_ I was completely outraged! I don't even eat ice cream because I'm lactose intolerant_comma_ and she knows that! I wonder who actually ate it then.\n",
      "Predicted Emotion:  guilty\n",
      "Actual Label:  furious\n",
      "-----------\n",
      "Sentence:  I found out that my childhood cat passed away yesterday  Thats sad. I love cats\n",
      "Predicted Emotion:  sad\n",
      "Actual Label:  sad\n"
     ]
    }
   ],
   "source": [
    "class HParams:\n",
    "  batch_size: int = 32\n",
    "  integer_input: bool = False\n",
    "  # cannot change, because en_core_web_md creates word vectors with 300 dimensions\n",
    "  word_vec_dimension: int = 300\n",
    "  num_classes: int = 32\n",
    "  learning_rate: float = 0.001\n",
    "  max_epochs: int = 4\n",
    "\n",
    "trainer(\n",
    "  model=WordVectorClassificationModel(HParams.word_vec_dimension,\n",
    "                                      HParams.num_classes),\n",
    "  params=HParams,\n",
    "  vectorizer=SpacyVectorizer())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
