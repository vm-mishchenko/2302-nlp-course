{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOTJFRLT5i1Y"
   },
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "\n",
    "# Week 3: Embedding-Based Retrieval\n",
    "\n",
    "### What we are building\n",
    "The goal of Embedding-Based Retrieval is to retrieve top-k candidates given a query based on embedding similarity/distance. A common application for this is given a query/sentence/document, find top-k similar candidates wrt query. While this is usually solved using TF-IDF/Information Retrieval (IR) based approaches, it is becoming more and more common in the industry to use an embedding based approach: encode the query and document as an embedding and use approximate nearest neighbor search to find top-k candidates in real-time.\n",
    "\n",
    "We will build a system to find duplicate questions on Quora using a [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs). A very common problem for forums/QA websites is trying to determine whether a question has already been asked before a user posts it.\n",
    "\n",
    "We will continue to apply our learning philosophy of repetition as we build multiple models of increasing complexity in the following order:\n",
    "\n",
    "1. Retrieval based on WordVectors\n",
    "1. Using BERT\n",
    "1. Using Sentence BERT\n",
    "1. Using Cohere Sentence Embeddings\n",
    "\n",
    "###  Evaluation\n",
    "We will evaluate our models along the following metrics: \n",
    "\n",
    "1. Recall@k: the proportion of relevant items found in the top-k matches\n",
    "2. Mean Reciprocal Rank: the rank of the first relevant item with respect to the top-k.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We have provide scaffolding for all the boiler plate Faiss code to get to our baseline model. This covers downloading and parsing the dataset, and training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point in our model, we will aim to use BERT embeddings. **Does this improve accuracy?**\n",
    "1. In the third model, we will use Sentence BERT and then we'll see if they can boost up our model. **How do you think this model will perform?**\n",
    "1. **Extension**: We have suggested a bunch of extensions to the project so go crazy! Tweak any parts of the pipeline, and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Project\n",
    "  - Dataset: Download the Quora dataset\n",
    "  - Indexer: Function to manage and create a Faiss Index\n",
    "  - Model 1: Word Vectors\n",
    "  - Model 2: BERT\n",
    "  - Model 3: Sentence BERT\n",
    "  - Model 4: Cohere Sentence Embeddings\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8zLCEfd7VKI"
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "✨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajhbV2UD5UGd"
   },
   "source": [
    "# Install all the required dependencies for the project\n",
    "!pip install pytorch-lightning==1.6.5\n",
    "!pip install spacy==2.2.4\n",
    "!python -m spacy download en_core_web_md\n",
    "!apt install libopenblas-base libomp-dev\n",
    "!pip install faiss==1.5.3\n",
    "!pip install faiss-cpu\n",
    "!pip install -U sentence-transformers\n",
    "!pip install transformers==4.17.0\n",
    "#!pip install sentence-transformers==2.2.0\n",
    "!pip install cohere"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning==1.6.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.6.5)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.5.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (0.11.1)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (23.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (6.0)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (2.11.2)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.64.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.21.6)\r\n",
      "Requirement already satisfied: torch>=1.8.* in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.13.1)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (2023.1.0)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (0.3.2)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.8.4)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2.28.2)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.4.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.2.3)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.8.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.4.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (60.2.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.51.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.6)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.37.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.16.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (6.0.4)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (0.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.0.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.8.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (22.2.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (4.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.1)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (5.3.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (4.9)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.16.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (6.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2022.12.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.1.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.13.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.2.2)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: spacy==2.2.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (2.2.4)\r\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (7.4.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (60.2.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (2.28.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (2.0.7)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.6)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (3.0.8)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.2)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.9)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (0.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.21.6)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (4.64.1)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (1.1.3)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy==2.2.4) (0.10.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.5.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.13.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting en_core_web_md==2.2.5\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\r\n",
      "     |████████████████████████████████| 96.4 MB 1.7 MB/s             \r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: spacy>=2.2.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from en_core_web_md==2.2.5) (2.2.4)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (60.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.6)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\r\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.8)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.7)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.10.1)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.64.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.28.2)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.9)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.5.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2022.12.7)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_md')\r\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n",
      "Requirement already satisfied: faiss==1.5.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.5.3)\r\n",
      "Requirement already satisfied: numpy in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from faiss==1.5.3) (1.21.6)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: faiss-cpu in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.7.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting sentence-transformers\r\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting transformers<5.0.0,>=4.6.0\r\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\r\n",
      "     |████████████████████████████████| 6.3 MB 4.0 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sentence-transformers) (4.64.1)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sentence-transformers) (1.13.1)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.14.1-cp37-cp37m-macosx_10_9_x86_64.whl (1.4 MB)\r\n",
      "     |████████████████████████████████| 1.4 MB 18.3 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: numpy in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\r\n",
      "Requirement already satisfied: scipy in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\r\n",
      "Collecting nltk\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "     |████████████████████████████████| 1.5 MB 17.1 MB/s            \r\n",
      "\u001B[?25hCollecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-macosx_10_9_x86_64.whl (1.2 MB)\r\n",
      "     |████████████████████████████████| 1.2 MB 15.4 MB/s            \r\n",
      "\u001B[?25hCollecting huggingface-hub>=0.4.0\r\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\r\n",
      "     |████████████████████████████████| 190 kB 14.0 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.0)\r\n",
      "Collecting filelock\r\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\r\n",
      "Collecting regex!=2019.12.17\r\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-macosx_10_9_x86_64.whl (294 kB)\r\n",
      "     |████████████████████████████████| 294 kB 17.2 MB/s            \r\n",
      "\u001B[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\r\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-macosx_10_11_x86_64.whl (3.8 MB)\r\n",
      "     |████████████████████████████████| 3.8 MB 17.8 MB/s            \r\n",
      "\u001B[?25hCollecting click\r\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\r\n",
      "Requirement already satisfied: joblib in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.13.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\r\n",
      "Building wheels for collected packages: sentence-transformers\r\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=d690a2840faede4b2aadae6305134c693d2da26c6f9eb39862e814ed3d219d29\r\n",
      "  Stored in directory: /Users/vitalii.mishchenko/Library/Caches/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\r\n",
      "Successfully built sentence-transformers\r\n",
      "Installing collected packages: filelock, tokenizers, regex, huggingface-hub, click, transformers, torchvision, sentencepiece, nltk, sentence-transformers\r\n",
      "Successfully installed click-8.1.3 filelock-3.9.0 huggingface-hub-0.12.1 nltk-3.8.1 regex-2022.10.31 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 torchvision-0.14.1 transformers-4.26.1\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting transformers==4.17.0\r\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\r\n",
      "     |████████████████████████████████| 3.8 MB 3.7 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (6.0)\r\n",
      "Collecting sacremoses\r\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\r\n",
      "     |████████████████████████████████| 880 kB 34.2 MB/s            \r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (3.9.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (2022.10.31)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (1.21.6)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (4.64.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (0.12.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (6.0.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (0.13.2)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (2.28.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from transformers==4.17.0) (23.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.5.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata->transformers==4.17.0) (3.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (3.0.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->transformers==4.17.0) (2022.12.7)\r\n",
      "Requirement already satisfied: six in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers==4.17.0) (1.16.0)\r\n",
      "Requirement already satisfied: click in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers==4.17.0) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from sacremoses->transformers==4.17.0) (1.2.0)\r\n",
      "Building wheels for collected packages: sacremoses\r\n",
      "  Building wheel for sacremoses (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b8883b359f6774b403c05940ce5f3eca591487b0be1f9830d09e7d2a8428121c\r\n",
      "  Stored in directory: /Users/vitalii.mishchenko/Library/Caches/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\r\n",
      "Successfully built sacremoses\r\n",
      "Installing collected packages: sacremoses, transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.26.1\r\n",
      "    Uninstalling transformers-4.26.1:\r\n",
      "      Successfully uninstalled transformers-4.26.1\r\n",
      "Successfully installed sacremoses-0.0.53 transformers-4.17.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: cohere in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (3.9.0)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from cohere) (2.28.2)\r\n",
      "Requirement already satisfied: urllib3~=1.26 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from cohere) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->cohere) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->cohere) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->cohere) (3.0.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTWZJAqiBxEv"
   },
   "source": [
    "Import all the necessary libraries we need throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "esA3TFU2-9dI"
   },
   "source": [
    "# Import all the relevant libraries\n",
    "import csv\n",
    "import en_core_web_md\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "import cohere\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, DistilBertTokenizer, DistilBertModel"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWE7zx6Wnria"
   },
   "source": [
    "Now let's load the Spacy data, which comes with pre-trainined embeddings. This process is expensive so only do it once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BhsykZdEK2m6"
   },
   "source": [
    "# Really expensive operation to load the entire space word-vector index in memory\n",
    "# We'll only run it once \n",
    "loaded_spacy_model = en_core_web_md.load()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiXEUsahCJeA"
   },
   "source": [
    "# Embedding Based Retrieval\n",
    "\n",
    "✨ Let's Begin ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFcN6rKkCQiu"
   },
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Download the duplicate questions [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nTXv0v34AYOU"
   },
   "source": [
    "!wget 'http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv'\n",
    "!mkdir qqp\n",
    "!mv quora_duplicate_questions.tsv qqp/\n",
    "!ls qqp/"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-02 15:04:31--  http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\r\n",
      "Resolving qim.fs.quoracdn.net (qim.fs.quoracdn.net)... 151.101.65.2, 151.101.129.2, 151.101.193.2, ...\r\n",
      "Connecting to qim.fs.quoracdn.net (qim.fs.quoracdn.net)|151.101.65.2|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 58176133 (55M) [text/tab-separated-values]\r\n",
      "Saving to: ‘quora_duplicate_questions.tsv’\r\n",
      "\r\n",
      "quora_duplicate_que 100%[===================>]  55.48M  56.7MB/s    in 1.0s    \r\n",
      "\r\n",
      "2023-03-02 15:04:34 (56.7 MB/s) - ‘quora_duplicate_questions.tsv’ saved [58176133/58176133]\r\n",
      "\r\n",
      "quora_duplicate_questions.tsv\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjVbbS-CucF0"
   },
   "source": [
    "Perfect. Now we see all of our files. Let's poke at one of them before we start parsing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S-MUggjUui6y"
   },
   "source": [
    "DATA_FILE = \"qqp/quora_duplicate_questions.tsv\"\n",
    "\n",
    "# The file is a 6-column tab separated file. \n",
    "# The first column is the row_id, second and third questions are ids of \n",
    "# specific questions, followed by the text of questions.\n",
    "# The last column captures if the two questions are duplicates\n",
    "with open(DATA_FILE, 'r', newline='\\n') as file:\n",
    "  reader = csv.reader(file, delimiter = '\\t')\n",
    "  # Read first 10 lines\n",
    "  for i in range(10):\n",
    "    print(next(reader))"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
      "['0', '1', '2', 'What is the step by step guide to invest in share market in india?', 'What is the step by step guide to invest in share market?', '0']\n",
      "['1', '3', '4', 'What is the story of Kohinoor (Koh-i-Noor) Diamond?', 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', '0']\n",
      "['2', '5', '6', 'How can I increase the speed of my internet connection while using a VPN?', 'How can Internet speed be increased by hacking through DNS?', '0']\n",
      "['3', '7', '8', 'Why am I mentally very lonely? How can I solve it?', 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?', '0']\n",
      "['4', '9', '10', 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?', 'Which fish would survive in salt water?', '0']\n",
      "['5', '11', '12', 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\", '1']\n",
      "['6', '13', '14', 'Should I buy tiago?', 'What keeps childern active and far from phone and video games?', '0']\n",
      "['7', '15', '16', 'How can I be a good geologist?', 'What should I do to be a great geologist?', '1']\n",
      "['8', '17', '18', 'When do you use シ instead of し?', 'When do you use \"&\" instead of \"and\"?', '0']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5uNuyGSxRql"
   },
   "source": [
    "The dataset has more than 500k questions! We are going to parse the full dataset and create a sample of 10k questions to experiment with in our models since BERT training & inference can be really slow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6_mRCola0s8z"
   },
   "source": [
    "\"\"\"\n",
    "Util function to parse the file\n",
    "\"\"\"\n",
    "def parse_sample_dataset(file_path, sample_max_id):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    file_path: Path to the raw data file\n",
    "    sample_max_id: Max question id to be considered in the sampled dataset\n",
    "\n",
    "  Returns 4 objects:\n",
    "    1. QuestionMap: list of all question ids\n",
    "    2. DuplicatesMap: Map of questionID to it's duplicates\n",
    "    3. SampleDataset: list of questionIds in the sample\n",
    "    4. SampleEvalDataset: list of pair of duplicate questions in the sample\n",
    "  \"\"\"\n",
    "  question_map = {}\n",
    "  duplicates_map = defaultdict(set)\n",
    "  sample_dataset = set([])\n",
    "  sample_eval_dataset = []\n",
    "\n",
    "  with open(file_path, 'r', newline='\\n') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader)  # Skip the header line\n",
    "\n",
    "    for row in reader:\n",
    "      if len(row) != 6: # Skip incomplete rows\n",
    "        continue\n",
    "\n",
    "      # Limit the sample size of the dataset at max_id\n",
    "      # Make sure all 4 objects start at index 0\n",
    "      qid1, qid2, label = int(row[1]) - 1, int(row[2]) - 1, int(row[5])\n",
    "      if qid1 < sample_max_id and qid2 < sample_max_id:\n",
    "        \n",
    "        if qid1 not in question_map:\n",
    "          question_map[qid1] = str(row[3])\n",
    "        if qid2 not in question_map:\n",
    "          question_map[qid2] = str(row[4])\n",
    "\n",
    "        if label == 1:\n",
    "          duplicates_map[qid1].add(qid2)\n",
    "          duplicates_map[qid2].add(qid1)\n",
    "\n",
    "          sample_eval_dataset.append((qid1, qid2))\n",
    "\n",
    "        sample_dataset.add(qid1)\n",
    "        sample_dataset.add(qid2)\n",
    "\n",
    "  # sample dataset duplicates removed via set(), so turn back into list\n",
    "  return question_map, duplicates_map, list(sample_dataset), sample_eval_dataset\n",
    "\n",
    "sample_max_id = 10000 # original\n",
    "# sample_max_id = 2000 # for quick development\n",
    "question_map, duplicates_map, sample_dataset, sample_eval_dataset, = parse_sample_dataset(DATA_FILE, sample_max_id)\n",
    "\n",
    "# Complete file: 537k unique questions, 400k duplicate.\n",
    "# To keep training time manageable limited to 10.000 (sample_max_id)\n",
    "print(\"Number of unique questions:\", len(question_map)) # 10.000\n",
    "print(\"Number of question with duplicates:\", len(duplicates_map)) # ~3.8k\n",
    "print(\"Number of questions in sample:\", len(sample_dataset)) # 10.000\n",
    "print(\"Number of duplicate pairs in sample:\", len(sample_eval_dataset)) # ~3.6k"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions: 10000\n",
      "Number of question with duplicates: 3810\n",
      "Number of questions in sample: 10000\n",
      "Number of duplicate pairs in sample: 3589\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC9D41185-Oa"
   },
   "source": [
    "# Retrieval using Faiss -- COMPLETED\n",
    "\n",
    "You are now going to create an Indexer class that implements multiple functions for indexing, searching, and evaluating our retrieval model. Faiss documentation can be found in the wiki here: https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
    "\n",
    "Some helpful Faiss guides are:\n",
    "- https://www.pinecone.io/learn/faiss-tutorial/\n",
    "- https://www.pinecone.io/learn/vector-indexes/\n",
    "\n",
    "You need to implement the following functions:\n",
    "\n",
    "1. **search**: Implement a function that takes a question and top_k variable and returns either the matched strings or the ids to the user as a \n",
    "    1. Call the search API on the faiss_index to look up similar sentences using `faiss_index.search`\n",
    "    2. Parse the output to either return [sentence_id, score] tuples or [sentence, score] tuples based on the input parameter\n",
    "    3. Sort the output by the score in descending order\n",
    "\n",
    "1. **evaluate**: Sample num_docs pairs from the evaluation dataset and then check if the qid2 is present in the top-k results\n",
    "    1. For each eval sample, find the top_k matches for the qid1\n",
    "    2. See if the qid2 is in one of the matches\n",
    "    3. If yes, append (1) to the recall array otherwise append (0)\n",
    "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ji7yyBk5Ou6k"
   },
   "source": [
    "class FaissIndexer:\n",
    "  def __init__(self, dataset,\n",
    "               question_map, \n",
    "               eval_dataset, \n",
    "               batch_size, \n",
    "               sentence_vector_dim, \n",
    "               vectorizer):\n",
    "    self.dataset = dataset\n",
    "    self.question_map = question_map\n",
    "    self.eval_dataset = eval_dataset\n",
    "    self.batch_size = batch_size\n",
    "    self.faiss_index = faiss.IndexFlatIP(sentence_vector_dim) # FlatIP uses L2 distance\n",
    "    self.vectorizer = vectorizer\n",
    "\n",
    "\n",
    "  def index(self):\n",
    "    sentence_vectors = []\n",
    "\n",
    "    print(\"Start indexing!\")\n",
    "    # tqdm - shows loop progress (https://tqdm.github.io)\n",
    "    for sentence_ids in tqdm(self.split_list_(self.dataset, self.batch_size)):\n",
    "      # Retrieve sentences based on qid\n",
    "      sentences = [question_map[qid] for qid in sentence_ids]\n",
    "      # Get embeddings of the sentences (Spacy, ..., Cohere)\n",
    "      sentence_vectors_batch = self.vectorizer.vectorize(sentences)\n",
    "      # Add batch to temporary list\n",
    "      sentence_vectors.append(sentence_vectors_batch)\n",
    "\n",
    "    # Add all batches from temporary list to index\n",
    "    self.faiss_index.add(np.array(np.concatenate(sentence_vectors, axis=0)))\n",
    "    print(\"\\nDone indexing!\")\n",
    "\n",
    "\n",
    "  def split_list_(self, lst: list, sublist_size: int):\n",
    "    sublists = []\n",
    "    # Split list into even chunks/sublists/batches\n",
    "    for i in range(0, len(lst), sublist_size):\n",
    "      sublists.append(lst[i:i + sublist_size])\n",
    "    return sublists\n",
    "\n",
    "\n",
    "  def search(self, question: str, top_k: int, return_ids=False):\n",
    "    \"\"\"Given any sentence (typed by the user)\n",
    "    We return a list of top_k(sentence, sim_score) or top_k(sentence_ids, sim_score)\n",
    "    \n",
    "    NOTE: The output type is controlled by the return_ids flag\n",
    "\n",
    "    1. Call the search API on the faiss_index to look up similar sentences \n",
    "       using `faiss_index.search`\n",
    "    2. Parse the output to either return [sentence_id, score] tuples or \n",
    "       [sentence, score] tuples based on return_ids being true/false\n",
    "    3. Sort the output by the score in descending order\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: We converted the question to a list here to match the signature \n",
    "    # of the vectorize function\n",
    "    question_vectors = self.vectorizer.vectorize([question])\n",
    "    scores, indices = self.faiss_index.search(np.array(question_vectors), top_k)\n",
    "\n",
    "    # Output is a List[(qid, score), (qid, score), (qid, score)] or\n",
    "    # List[(q, score), (q, score), (q, score)] based on return_ids\n",
    "    # Output is sorted in descending order of score\n",
    "    if return_ids == True:\n",
    "      output = list(zip(indices[0], scores[0]))\n",
    "    else:\n",
    "      output = [(self.question_map[qid], sentence) for qid, sentence in zip(indices[0], scores[0])]\n",
    "    output.sort(reverse=True, key=lambda pair: pair[1])\n",
    "    return output\n",
    "\n",
    "\n",
    "  def evaluate(self, top_k: int, eval_sample_size: int):\n",
    "    \"\"\"Sample num_docs pairs from the evaluation dataset and then check \n",
    "    if the qid2 is present in the top-k results\n",
    "\n",
    "    1. For each eval sample, find the top_k matches for the qid1\n",
    "    2. See if the qid2 is in one of the matches\n",
    "    3. If yes, append (1) to the recall array otherwise append (0)\n",
    "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches\n",
    "      - Note: MRR is equivalent to mean([1/r or 0 for each sample])\n",
    "    \"\"\"\n",
    "    # Sample from evaluation dataset as proxy for performance metrics\n",
    "    eval_samples = random.sample(self.eval_dataset, eval_sample_size)\n",
    "\n",
    "    # Retrieval metrics which only care about if searched for\n",
    "    # item is present among the results.\n",
    "    recall_at_k = [] # Relevant items vs total of relevant items\n",
    "    mean_reciprocal_rank = [] # Rank of the first relevant item\n",
    "\n",
    "    for eval_sample in eval_samples:\n",
    "      first_qid = eval_sample[0]\n",
    "      second_qid = eval_sample[1]\n",
    "      first_question = self.question_map[first_qid]\n",
    "      search_results = self.search(first_question, top_k, return_ids=True)\n",
    "\n",
    "      result_qids = [qid for (qid, _) in search_results]\n",
    "      if second_qid in result_qids:\n",
    "        recall_at_k.append(1)\n",
    "\n",
    "        second_q_position_in_results = result_qids.index(second_qid)\n",
    "        reciprocal = 1 / (second_q_position_in_results + 1)\n",
    "        mean_reciprocal_rank.append(reciprocal)\n",
    "      else:\n",
    "        recall_at_k.append(0)\n",
    "        mean_reciprocal_rank.append(0)\n",
    "\n",
    "    recall = np.mean(np.array(recall_at_k) * 100.0)\n",
    "    reciprocal_rank = np.mean(np.array(mean_reciprocal_rank))\n",
    "    print(\"\\nRecall@{}:\\t\\t{:0.2f}%\".format(top_k, recall))\n",
    "    print(\"Mean Reciprocal Rank:\\t{:0.2f}\".format(reciprocal_rank))\n",
    "\n",
    "\n",
    "  # Helper function to train, search and evaluate similar output from all the models created.\n",
    "  def train_and_evaluate(self, \n",
    "                         question_example: str, \n",
    "                         top_k: int = 10, \n",
    "                         eval_sample_size: int = 1000\n",
    "                         ):\n",
    "    print(\"---- Indexing ----\")\n",
    "    self.index()\n",
    "    print(\"\\n---- Search ----\")\n",
    "    results = self.search(question_example, top_k, return_ids=False)\n",
    "    print(\"Questions similar to:\", question_example)\n",
    "    for i, (q, s) in enumerate(results):\n",
    "      print(f\"{i} Question: {q} with score {s}\")\n",
    "    print(\"\\n---- Evaluation ----\")\n",
    "    self.evaluate(top_k, eval_sample_size)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuMWzJZjpKdW"
   },
   "source": [
    "## Dummy Model Test\n",
    "\n",
    "Really small sample of 4 sentences to make sure we can test our implementation of the FAISS search function correctly. We just project the 4 questions in a 2-d space where they are placed on the X-Axis if the word `invest` is present and on the Y-axis if `kohinoor` is present. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UmvNFIIw1eO"
   },
   "source": [
    "dummy_ids = sample_dataset[:4]\n",
    "print(\"Questions:\")\n",
    "for i in dummy_ids:\n",
    "  print(i, \":\", question_map[i])"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "0 : What is the step by step guide to invest in share market in india?\n",
      "1 : What is the step by step guide to invest in share market?\n",
      "2 : What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "3 : What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x8n4UBEZpPT1"
   },
   "source": [
    "class DummyVectorizer:\n",
    "  def __init__(self, sentence_vector_dim):\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "      if \"invest\" in sentence:\n",
    "        # If \"invest\" is present place it on the X-Axis\n",
    "        vectors.append(np.array([random.random(), 0], dtype=np.float32))\n",
    "      elif \"Kohinoor\" in sentence:\n",
    "        # If \"Kohinoor\" is present place it on the Y-Axis\n",
    "        vectors.append(np.array([0, random.random()], dtype=np.float32))\n",
    "    return np.stack(vectors)\n",
    "\n",
    "\n",
    "di = FaissIndexer(dummy_ids, \n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024, \n",
    "                  sentence_vector_dim=2, \n",
    "                  vectorizer=DummyVectorizer(2)\n",
    "                  )\n",
    "\n",
    "di.index()\n",
    "\n",
    "results = di.search(\"invest\", 4)\n",
    "print(\"Questions similar to:\", \"invest\")\n",
    "for i, (q, s) in enumerate(results):\n",
    "  print(f\"{i} Question: {q} with score {s}\")\n",
    "\n",
    "results = di.search(\"Kohinoor\", 4)\n",
    "print(\"\\nQuestions similar to:\", \"Kohinoor\")\n",
    "for i, (q, s) in enumerate(results):\n",
    "  print(f\"{i} Question: {q} with score {s}\")"
   ],
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 2702.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "Questions similar to: invest\n",
      "0 Question: What is the step by step guide to invest in share market? with score 0.03926607966423035\n",
      "1 Question: What is the step by step guide to invest in share market in india? with score 0.01793918013572693\n",
      "2 Question: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back? with score 0.0\n",
      "3 Question: What is the story of Kohinoor (Koh-i-Noor) Diamond? with score 0.0\n",
      "\n",
      "Questions similar to: Kohinoor\n",
      "0 Question: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back? with score 0.6463944315910339\n",
      "1 Question: What is the story of Kohinoor (Koh-i-Noor) Diamond? with score 0.08856147527694702\n",
      "2 Question: What is the step by step guide to invest in share market? with score 0.0\n",
      "3 Question: What is the step by step guide to invest in share market in india? with score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtZuKF-Y3kkB"
   },
   "source": [
    "# Models\n",
    "\n",
    "You may be wondering, \"When are we going to start building models?\" And, the answer is NOW! Finally the time has come to build our baseline model, and then we'll work towards improving it. \n",
    "\n",
    "\n",
    "**NOTE**: We will be using the sample dataset since BERT is really slow and processing the full dataset will take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFesUhmewgGY"
   },
   "source": [
    "### Model 1: Averaging Word Vectors --- COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~20%, MRR: ~0.07</font>\n",
    "\n",
    "Complete the `vectorize` function using Spacy provided word embeddings. This is something we've done twice already :) \n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Tokenize each sentence and get wordVectors for each token in the sentence using Spacy \n",
    "2. Sentence vector is the mean of word vectors of each token\n",
    "3. Stack the sentence vectors into a numpy array using np.stack"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ahi3pH_Ce6c"
   },
   "source": [
    "class SpacyVectorizer:\n",
    "  def __init__(self, sentence_vector_dim):\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "      tokens = loaded_spacy_model(sentence)\n",
    "      token_vectors = []\n",
    "      for token in tokens:\n",
    "        token_vectors.append(token.vector)\n",
    "\n",
    "      sentence_vector = np.mean(np.array(token_vectors), axis=0)\n",
    "      vectors.append(sentence_vector)\n",
    "    return np.stack(vectors)\n",
    "\n",
    "\n",
    "spacyIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024, \n",
    "                  sentence_vector_dim=300, \n",
    "                  vectorizer=SpacyVectorizer(300))\n",
    "\n",
    "spacyIndex.index()\n",
    "spacyIndex.search(\"how can i invest in stock market in india?\", 10)\n",
    "\n",
    "sample_size = 1000\n",
    "top_k_for_each_sample = 10\n",
    "spacyIndex.evaluate(top_k_for_each_sample, sample_size)"
   ],
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:54<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "Recall@10:\t\t20.70%\n",
      "Mean Reciprocal Rank:\t0.07\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHoRuwCOwhiH"
   },
   "source": [
    "### Model 2: BERT Embeddings --- COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~48%, MRR: ~0.19</font>\n",
    "\n",
    "Compute the sentence embeddings using the BERT model and complete the `vectorize` function. Feel free to reference any documentation from https://huggingface.co/. \n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Tokenize batch of sentences using `self.tokenizer`\n",
    "2. Pipe the inputs through the BERT model to create the output logits\n",
    "3. Normalize the batch output\n",
    "\n",
    "**NOTE: This model is really slow and will take about 20 mins to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [14:18<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: I wish to start investing in Equity and Mutual Funds. Where should I open Demat account for best rates, transaction charges and so on? I am NRI. with score 0.8770731091499329\n",
      "1 Question: What is the step by step guide to invest in share market in india? with score 0.8744895458221436\n",
      "2 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.8723897933959961\n",
      "3 Question: What will be the effect of banning 500 and 1000 notes on stock markets in India? with score 0.8636164665222168\n",
      "4 Question: What will be the effect of banning 500 and 1000 Rs notes on real estate sector in India? Can we expect sharp fall in prices in short/long term? with score 0.8614913821220398\n",
      "5 Question: What are your views on Modi governments decision to demonetize 500 and 1000 rupee notes? How will this affect economy? with score 0.8532259464263916\n",
      "6 Question: What is the best time to invest in real estate market after demonetisation? with score 0.8521274328231812\n",
      "7 Question: Do you think India will be able to curb blank money? with score 0.8468541502952576\n",
      "8 Question: What should I do to make money online in India? with score 0.8458528518676758\n",
      "9 Question: How much does an Ola Mini cab earn on average per month from an investor's point of view in Mumbai? with score 0.8440941572189331\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t49.20%\n",
      "Mean Reciprocal Rank:\t0.20\n"
     ]
    }
   ],
   "source": [
    "class BertVectorizer:\n",
    "  def __init__(self):\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    self.model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences.\n",
    "\n",
    "    1. Tokenize batch of sentences using `self.tokenizer`\n",
    "    2. Pipe the inputs through the BERT model to create the output logits\n",
    "    3. Normalize the batch output\n",
    "    \"\"\"\n",
    "\n",
    "    # converts words to IDs\n",
    "    tokens = self.tokenizer(\n",
    "      sentences,\n",
    "      padding=True,\n",
    "      return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # BERT model expects input_ids\n",
    "    # https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertModel.forward\n",
    "    outputs = self.model(**tokens)\n",
    "    model_output = outputs['last_hidden_state'].detach()\n",
    "\n",
    "    return F.normalize(torch.mean(model_output, dim=1), dim=1).detach().numpy()\n",
    "\n",
    "\n",
    "bertIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=32,\n",
    "                  sentence_vector_dim=768,\n",
    "                  vectorizer=BertVectorizer())\n",
    "\n",
    "bertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bLdQ4pQwo-m"
   },
   "source": [
    "### Model 3: Sentence Transformer --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~93%, MRR: ~0.34</font>\n",
    "\n",
    "Compute the sentence embeddings using the Sentence BERT model and complete the `vectorize` function. Feel free to look up documentation on https://www.sbert.net/. \n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
    "2. Normalize the batch output\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IpTt--KFTd3t"
   },
   "source": [
    "class SentenceBertVectorizer:\n",
    "  def __init__(self):\n",
    "    self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
    "    2. Normalize the batch output\n",
    "    \"\"\"\n",
    "    sentence_vectors = self.model.encode(sentences)\n",
    "\n",
    "    return sentence_vectors / np.expand_dims(np.linalg.norm(sentence_vectors, axis=1), axis=1)\n",
    "\n",
    "\n",
    "SBertIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024, \n",
    "                  sentence_vector_dim=384, \n",
    "                  vectorizer=SentenceBertVectorizer())\n",
    "\n",
    "SBertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)001fa/.gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe2274adb79540fd9cf5edd80fefdabb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "122f13ccbc974efabb61df03c9f8b772"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)3bbb8001fa/README.md:   0%|          | 0.00/3.69k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8e5df8911f04867b71dd20406d17e0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)bb8001fa/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8260457c08b94be6b63c97bc3faf2d1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6d48f0e0db24eec968c3ca864c64ac2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64d8ee7d221443178cc243596c2c8093"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7eb9010c3fc249b5a02ad6d9c60c7a3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a4ed9d0e8d7496393564c03614224cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)001fa/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8860e8eb0c694f809deb11cb198a0454"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd5045f77fa74da3b6c0bfbc453ba0e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)3bbb8001fa/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38f55df9ca804c458ac3675baeb0ca78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)b8001fa/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c814828b13504666a356bab746f3e59f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:10<00:00, 13.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 0.733176589012146\n",
      "1 Question: I am 17 and I want to invest money in stock market where should I start? with score 0.6957336664199829\n",
      "2 Question: What are the ways to learn about stock market? with score 0.6243616342544556\n",
      "3 Question: How do I start investing in shares or stocks? What is the minimum requirement? with score 0.6239825487136841\n",
      "4 Question: What is the best way to learn about stock market? with score 0.6222878694534302\n",
      "5 Question: What is the step by step guide to invest in share market? with score 0.6042821407318115\n",
      "6 Question: What is the best way to learn about investing in the stock market and what stocks to buy? with score 0.6032654643058777\n",
      "7 Question: What is the best way to learn about stock markets? with score 0.5846710205078125\n",
      "8 Question: How do I buy stocks? with score 0.5778073072433472\n",
      "9 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.5576666593551636\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t91.80%\n",
      "Mean Reciprocal Rank:\t0.35\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHhRKpknq5we"
   },
   "source": [
    "### Model 4: Cohere Sentence Embeddings --- COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~89%, MRR: ~0.34</font>\n",
    "\n",
    "Make sure create a Cohere account and make an API key.\n",
    "Compute the sentence embeddings using the cohere API and complete the `vectorize` function. Feel free to look up documentation on https://docs.cohere.ai/semantic-search. \n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Pipe the input sentences through the Cohere API. Make sure to select the small model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# https://dashboard.cohere.ai/api-keys\n",
    "COHERE_API_KEY = \"\"\n",
    "co = cohere.Client(COHERE_API_KEY)"
   ],
   "metadata": {
    "id": "FODgVGURr2YP"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import functools as _functools\n",
    "import threading as _threading\n",
    "\n",
    "def limit(limit, every=1):\n",
    "  \"\"\"This decorator factory creates a decorator that can be applied to\n",
    "     functions in order to limit the rate the function can be invoked.\n",
    "     The rate is `limit` over `every`, where limit is the number of\n",
    "     invocation allowed every `every` seconds.\n",
    "     limit(4, 60) creates a decorator that limit the function calls\n",
    "     to 4 per minute. If not specified, every defaults to 1 second.\"\"\"\n",
    "\n",
    "  def limitdecorator(fn):\n",
    "    \"\"\"This is the actual decorator that performs the rate-limiting.\"\"\"\n",
    "    semaphore = _threading.Semaphore(limit)\n",
    "\n",
    "    @_functools.wraps(fn)\n",
    "    def wrapper(*args, **kwargs):\n",
    "      semaphore.acquire()\n",
    "\n",
    "      try:\n",
    "        return fn(*args, **kwargs)\n",
    "\n",
    "      finally:                   # ensure semaphore release\n",
    "        timer = _threading.Timer(every, semaphore.release)\n",
    "        timer.setDaemon(True)  # allows the timer to be canceled on exit\n",
    "        timer.start()\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "  return limitdecorator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QnYpcV8Cq5wk"
   },
   "source": [
    "class CohereVectorizer:\n",
    "  @limit(50, 60) # 50 calls in 60 seconds\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences. \n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the embeddings of our sentences by calling\n",
    "    # the API of Cohere.\n",
    "    sentence_vectors = co.embed(texts = sentences,\n",
    "                      model = \"small\",\n",
    "                      truncate = \"LEFT\").embeddings\n",
    "\n",
    "\n",
    "    # Convert from float64 to float32 to prevent bug:\n",
    "    # https://github.com/facebookresearch/faiss/issues/461\n",
    "    return np.float32(np.stack(sentence_vectors))\n",
    "\n",
    "\n",
    "cohereIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=32, \n",
    "                  sentence_vector_dim=1024, \n",
    "                  vectorizer=CohereVectorizer())\n",
    "\n",
    "cohereIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [06:06<00:00,  1.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 2562.99462890625\n",
      "1 Question: I am 17 and I want to invest money in stock market where should I start? with score 2064.28125\n",
      "2 Question: What is the step by step guide to invest in share market? with score 2049.330810546875\n",
      "3 Question: How do I start investing in shares or stocks? What is the minimum requirement? with score 1887.562744140625\n",
      "4 Question: Which is the best Mutual Fund in India? with score 1856.3857421875\n",
      "5 Question: I wish to start investing in Equity and Mutual Funds. Where should I open Demat account for best rates, transaction charges and so on? I am NRI. with score 1831.635986328125\n",
      "6 Question: How do I buy stocks? with score 1825.7650146484375\n",
      "7 Question: What are mutual funds and which is the best one in India in which to invest? with score 1824.3251953125\n",
      "8 Question: Which Best SIP plan in india for investement purpose? with score 1824.16455078125\n",
      "9 Question: What are the ways to learn about stock market? with score 1807.7784423828125\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t90.10%\n",
      "Mean Reciprocal Rank:\t0.34\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onEbNcqnq5wl"
   },
   "source": [
    "🎉 CONGRATULATIONS on finishing the assignment!!! We built a real model with an actual datasets for a problem that is used every time a new Quora question gets created!! \n",
    "\n",
    "As for why did SentenceBERT & Cohere perform so well, we'll cover that in Siamese networks in week4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM0pHSbtGHuj"
   },
   "source": [
    "# Extensions\n",
    "\n",
    "Now that you've worked through the project there is a lot more for us to try:\n",
    "\n",
    "- See if you can use BERT to improve the model you shipped in Week 1.\n",
    "  - Improved result on 2%. Take a look at \"text-sentiment-bert.ipynb\".\n",
    "- Try out `SentenceBert` and `SpacyVectors` on the entire dataset rather the sample and see what you get?\n",
    "- Try different transformer models from hugging face"
   ]
  }
 ]
}
