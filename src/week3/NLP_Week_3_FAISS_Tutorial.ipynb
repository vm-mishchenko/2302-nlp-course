{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "\n",
    "# Week 3: FAISS Tutorial\n",
    "\n",
    "### What we are looking at\n",
    "The goal of this small tutorial, is to provide you a quick overview into what FAISS does and how you can utilize it for Week 3 project. FAISS is an index for efficiently storing searchable embeddings of objects (e.g. sentences, images, ...). This efficient storing allows us to quickly compare our current object against the objects present in the index, and thus find relevant similar results. FAISS uses approximate nearest neighbor search to achieve these quick results.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Go through all the steps and look at what kind of embeddings we create.\n",
    "1. Feel free to add more sentences to be embedded.\n",
    "1. Make sure to have a look at the interactive graph, and see how close some results are, and how some are not. Does it make sense?\n",
    "1. Have a look at the results retrieved from the FAISS index we made. Are they appropriate? Try and play around with the number of results it retrieves.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Dataset creation\n",
    "- Cohere API\n",
    "- Creating a FAISS index\n"
   ],
   "metadata": {
    "id": "D3Q13pqVM-8D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "\n",
    "✨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ],
   "metadata": {
    "id": "faxj21eGPeQh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FoH9De1v8-ez"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\r\n",
      "  Using cached cohere-3.9.0-cp37-cp37m-macosx_10_9_x86_64.whl\r\n",
      "Requirement already satisfied: umap-learn in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (0.5.3)\r\n",
      "Requirement already satisfied: faiss-cpu in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (1.7.3)\r\n",
      "Collecting altair\r\n",
      "  Using cached altair-4.2.2-py3-none-any.whl (813 kB)\r\n",
      "Requirement already satisfied: urllib3~=1.26 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from cohere) (1.26.14)\r\n",
      "Requirement already satisfied: requests in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from cohere) (2.28.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from umap-learn) (4.64.1)\r\n",
      "Requirement already satisfied: numba>=0.49 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from umap-learn) (0.56.4)\r\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from umap-learn) (0.5.8)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from umap-learn) (1.21.6)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from umap-learn) (1.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from umap-learn) (1.7.3)\r\n",
      "Requirement already satisfied: entrypoints in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from altair) (0.4)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from altair) (4.17.3)\r\n",
      "Requirement already satisfied: toolz in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from altair) (0.12.0)\r\n",
      "Requirement already satisfied: pandas>=0.18 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from altair) (1.1.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from altair) (3.1.2)\r\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (1.3.10)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (0.19.3)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (22.2.0)\r\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (5.10.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (4.5.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (6.0.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (60.2.0)\r\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (0.39.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas>=0.18->altair) (2022.7.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pandas>=0.18->altair) (2.8.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from jinja2->altair) (2.1.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->cohere) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->cohere) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from requests->cohere) (3.0.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0->altair) (3.13.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.18->altair) (1.16.0)\r\n",
      "Installing collected packages: cohere, altair\r\n",
      "Successfully installed altair-4.2.2 cohere-3.9.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere umap-learn faiss-cpu altair\n",
    "!apt install libopenblas-base libomp-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all the necessary libraries we need throughout the project. Make sure to create a Cohere account and create an API key: https://os.cohere.ai/"
   ],
   "metadata": {
    "id": "vkBg-06IPkop"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cohere\n",
    "import umap\n",
    "import faiss\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# https://dashboard.cohere.ai/api-keys\n",
    "COHERE_API_KEY = \"\"\n",
    "co = cohere.Client(COHERE_API_KEY)"
   ],
   "metadata": {
    "id": "wPNkMb399Ngh"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset creation\n",
    "\n",
    "Below we create our own small dataset, and its WONDERFUL🤩. Please feel free to add your own examples to it, the more the better✨✨! We make use of Cohere to quickly retrieve sentence embeddings that can be used for storing in our FAISS index."
   ],
   "metadata": {
    "id": "vy8qFO6RPnzc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sentences = [\n",
    "             # Movies\n",
    "             \"I am watching a movie.\",\n",
    "             \"I'm going to the movies.\",\n",
    "             \"Cinema's popcorn smell is amazing.\",\n",
    "             \"These guys kept talking while I was watching the movie.\",\n",
    "             # Groceries\n",
    "             \"Groceries are expensive now?\",\n",
    "             \"What happend to all my groceries, they are all rotten.\",\n",
    "             \"I like avocado toast\",\n",
    "             \"Cheese is over there!\",\n",
    "             \"Spinach is the food of the gods.\",\n",
    "             \"Healthy dose of protein powder is always good.\",\n",
    "             # Music\n",
    "             \"Coldplay is not my favorite band anymore.\",\n",
    "             \"I really liked MTV, with all the video clips.\",\n",
    "             \"What music would you like me to play?\",\n",
    "             \"He's playing piano very well.\"\n",
    "             ]\n",
    "\n",
    "df = pd.DataFrame (sentences, columns = ['conversation'])\n",
    "df.head()"
   ],
   "metadata": {
    "id": "24IDW29Q-K4l"
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        conversation\n0                             I am watching a movie.\n1                           I'm going to the movies.\n2                 Cinema's popcorn smell is amazing.\n3  These guys kept talking while I was watching t...\n4                       Groceries are expensive now?",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I am watching a movie.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm going to the movies.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cinema's popcorn smell is amazing.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>These guys kept talking while I was watching t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Groceries are expensive now?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cohere API: U there?\n",
    "Here we retrieve the sentence embeddings through Cohere API. Be sure to check out the documentation: https://docs.cohere.ai/api-reference/"
   ],
   "metadata": {
    "id": "GIzW7VIwP-Vx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrieve the embeddings of our sentences by calling\n",
    "# the API of Cohere.\n",
    "embeds = co.embed(texts = sentences,\n",
    "                       model = \"small\", \n",
    "                       truncate = \"LEFT\").embeddings\n",
    "\n",
    "embeds = np.array(embeds)\n",
    "embeds.shape"
   ],
   "metadata": {
    "id": "cL3MSRDMG-xo"
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(14, 1024)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we make use of UMAP and altair. UMAP we use to reduce the dimensions of our embeddings (Small size is 1024 😵). No other way to plot it then using dimensionality reduction. With Altair we make an interactive plot.\n",
    "\n",
    "\n",
    "Please hover over some of these points and see if you can identify a pattern."
   ],
   "metadata": {
    "id": "vWVGOqrCRbRl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# UMAP reduces dimensions from 1024 to 2, which we can plot\n",
    "reducer = umap.UMAP()\n",
    "umap_embeds = reducer.fit_transform(embeds)\n",
    "# Make interactive plot\n",
    "df_explore = pd.DataFrame(data={'text': df['conversation']})\n",
    "df_explore['x'] = umap_embeds[:,0]\n",
    "df_explore['y'] = umap_embeds[:,1]\n",
    "chart = alt.Chart(df_explore).mark_circle(size=60).encode(\n",
    "    x=alt.X('x', scale=alt.Scale(zero=False)),\n",
    "    y=alt.Y('y', scale=alt.Scale(zero=False)),\n",
    "    tooltip=['text']\n",
    ").properties(width=700, height=400)\n",
    "chart.interactive()"
   ],
   "metadata": {
    "id": "pg1WbkT1HvqC"
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitalii.mishchenko/Documents/experiments/2302-nlp-course/venv/lib/python3.7/site-packages/umap/umap_.py:2345: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  \"n_neighbors is larger than the dataset size; truncating to \"\n"
     ]
    },
    {
     "data": {
      "text/html": "\n<div id=\"altair-viz-e8430b8f38e5458ca2dbca7508a6db6b\"></div>\n<script type=\"text/javascript\">\n  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-e8430b8f38e5458ca2dbca7508a6db6b\") {\n      outputDiv = document.getElementById(\"altair-viz-e8430b8f38e5458ca2dbca7508a6db6b\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n    };\n\n    function maybeLoadScript(lib, version) {\n      var key = `${lib.replace(\"-\", \"\")}_version`;\n      return (VEGA_DEBUG[key] == version) ?\n        Promise.resolve(paths[lib]) :\n        new Promise(function(resolve, reject) {\n          var s = document.createElement('script');\n          document.getElementsByTagName(\"head\")[0].appendChild(s);\n          s.async = true;\n          s.onload = () => {\n            VEGA_DEBUG[key] = version;\n            return resolve(paths[lib]);\n          };\n          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n          s.src = paths[lib];\n        });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else {\n      maybeLoadScript(\"vega\", \"5\")\n        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-82941d3e4797f176d7f1e4271b8a2c1c\"}, \"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"tooltip\": [{\"field\": \"text\", \"type\": \"nominal\"}], \"x\": {\"field\": \"x\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"y\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}}, \"height\": 400, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-82941d3e4797f176d7f1e4271b8a2c1c\": [{\"text\": \"I am watching a movie.\", \"x\": 1.8480154275894165, \"y\": 7.086116313934326}, {\"text\": \"I'm going to the movies.\", \"x\": 2.1247379779815674, \"y\": 7.8059844970703125}, {\"text\": \"Cinema's popcorn smell is amazing.\", \"x\": 0.539131224155426, \"y\": 7.526027202606201}, {\"text\": \"These guys kept talking while I was watching the movie.\", \"x\": 1.3701128959655762, \"y\": 7.711307525634766}, {\"text\": \"Groceries are expensive now?\", \"x\": 0.05854678899049759, \"y\": 8.934335708618164}, {\"text\": \"What happend to all my groceries, they are all rotten.\", \"x\": 0.6695223450660706, \"y\": 8.984118461608887}, {\"text\": \"I like avocado toast\", \"x\": 1.4679322242736816, \"y\": 10.018346786499023}, {\"text\": \"Cheese is over there!\", \"x\": 1.4492231607437134, \"y\": 8.70042610168457}, {\"text\": \"Spinach is the food of the gods.\", \"x\": 1.3202636241912842, \"y\": 9.395014762878418}, {\"text\": \"Healthy dose of protein powder is always good.\", \"x\": 0.5759827494621277, \"y\": 9.804512977600098}, {\"text\": \"Coldplay is not my favorite band anymore.\", \"x\": 0.8462863564491272, \"y\": 8.345588684082031}, {\"text\": \"I really liked MTV, with all the video clips.\", \"x\": 1.1301683187484741, \"y\": 6.944214344024658}, {\"text\": \"What music would you like me to play?\", \"x\": 2.446632146835327, \"y\": 8.288034439086914}, {\"text\": \"He's playing piano very well.\", \"x\": 2.065369129180908, \"y\": 8.79338264465332}]}}, {\"mode\": \"vega-lite\"});\n</script>",
      "text/plain": "alt.Chart(...)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating FAISS: the good stuff.\n",
    "Creating FAISS is rather straightforward. \n",
    "1. Identify which index you want to use, with the dimension your embeddings have. \n",
    "1. Add all the embeddings you want to add.\n",
    "\n",
    "Since we made embeddings of sentences, we can now query this index with an example like *\"I like eating cabbage\"*. We turn this into a embedding and search for related sentences in our small index."
   ],
   "metadata": {
    "id": "IiHEqxq5R6Nd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create our Approximate Nearest Neighbour Index (ANN)\n",
    "# https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
    "faiss_index = faiss.IndexFlatIP(1024)\n",
    "\n",
    "# Convert from float64 to float32 to prevent bug:\n",
    "# https://github.com/facebookresearch/faiss/issues/461\n",
    "faiss_index.add(np.float32(np.stack(embeds)))\n",
    "\n",
    "# Create an embedding for our sentence\n",
    "embed = co.embed(texts = [\"I like eating cabbage!\"], \n",
    "                 model = \"small\",\n",
    "                 truncate = \"LEFT\").embeddings\n",
    "\n",
    "# Get the results\n",
    "scores, indices = faiss_index.search(np.float32(np.array(embed)), 5)\n",
    "\n",
    "# Print the results\n",
    "for indice, score in zip(indices[0], scores[0]):\n",
    "  print(sentences[indice], \"\\t\\t\\t\\t\", score)"
   ],
   "metadata": {
    "id": "SNXsPkwZJbET"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like avocado toast \t\t\t\t 1615.6025\n",
      "Spinach is the food of the gods. \t\t\t\t 1347.9637\n",
      "Cheese is over there! \t\t\t\t 970.3561\n",
      "Cinema's popcorn smell is amazing. \t\t\t\t 938.28845\n",
      "What happend to all my groceries, they are all rotten. \t\t\t\t 923.17236\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "✨ Tada ✨, hopefully the results match your expectations! \n",
    "\n",
    "🙌 Good luck with the project! 🙌"
   ],
   "metadata": {
    "id": "9QauKfJnS5lS"
   }
  }
 ]
}
