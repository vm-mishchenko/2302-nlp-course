{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOTJFRLT5i1Y"
   },
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive. \n",
    "\n",
    "\n",
    "# Week 2: Named Entity Recognition (NER)\n",
    "\n",
    "### What we are building\n",
    "We'll build a model to do NER on an open source MIT Movie Dataset. We will continue to apply our learning philosophy of repetition as we build multiple models of increasing complexity in the following order:\n",
    "\n",
    "1. Simple model to get a baseline\n",
    "1. RNN based model\n",
    "1. Using LSTM. Does it do better? \n",
    "1. Using Stacked LSTMs\n",
    "1. **Extension**: Explore different parameters, features, and metrics. \n",
    "\n",
    "###  Evaluation\n",
    "We will evaluate our models along the following two dimensions: \n",
    "\n",
    "1. Accuracy: the ratio of the number of correctly classified instances to the total number of instances. \n",
    "1. F1: since this is a multi-class classification problem, we'll add these metrics to each of the runs using the relevant functions from torchmetrics.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We've provide scaffolding for all the boiler plate Pytorch code to get to our baseline model. This covers downloading and parsing the dataset, and training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point, our model gets to an accuracy of about 0.78. After this we'll try to improve the model by using RNNs and LSTMs. **Does this improve accuracy?**\n",
    "1. In the next set of models, we'll stack the LSTM cells and see if they can boost up our model. **How do you think this model will perform?**\n",
    "1. **Extension**: We have suggested a bunch of extensions to the project so go crazy! Tweak any parts of the pipeline, and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Project\n",
    "  - Dataset: Download the MIT dataset, and parse it into a pytorch Dataset\n",
    "  - Trainer: Trainer function to help with multi-epoch training\n",
    "  - Model 0: All Zeros\n",
    "  - Model 1: RNN\n",
    "  - Model 2: Bidirectional RNN\n",
    "  - Model 2: LSTM\n",
    "  - Model 3: Stacked LSTMs\n",
    "  - More metrics\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8zLCEfd7VKI"
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "âœ¨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tjbNgyV44rbg"
   },
   "source": [
    "# Install all the required dependencies for the project\n",
    "!pip install pytorch-lightning==1.6.5\n",
    "!pip install spacy==2.2.4\n",
    "!python -m spacy download en_core_web_md\n",
    "!pip install scikit-learn==1.0.2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTWZJAqiBxEv"
   },
   "source": [
    "Import all the necessary libraries we need throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CQKGy3Um4whH"
   },
   "source": [
    "# Import all the relevant libraries\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torchmetrics"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbxyFzWeB3O2"
   },
   "source": [
    "Now let's load the Spacy data, which comes with pre-trained embeddings. This process is expensive so only do this once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xuWw6ILAB36m"
   },
   "source": [
    "# Really expensive operation to load the entire space word-vector index in memory\n",
    "# We'll only run it once \n",
    "loaded_spacy_model = en_core_web_md.load()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQzYtuYZCAVw"
   },
   "source": [
    "Fix the random seed for numpy and pytorch so the entire class gets consistent results which. We can discuss this with each other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Aql6O1sECBIz"
   },
   "source": [
    "# Fix the random seed so that we get consistent results\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiXEUsahCJeA"
   },
   "source": [
    "# Named Entity Recognition Project \n",
    "\n",
    "âœ¨ Let's Begin âœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFcN6rKkCQiu"
   },
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Weâ€™ll be using the [MIT Movie dataset](https://groups.csail.mit.edu/sls/downloads/movie/). **Extension**: You can find a lot of other entity recognition datasets listed on Github. Play around with them to see how you perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hV1O-JfCdAY5"
   },
   "source": [
    "# Download the dataset and move it to a separate folder\n",
    "!mkdir -p mit_ner\n",
    "!wget 'https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio' -P './mit_ner'\n",
    "!wget 'https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio' -P './mit_ner'\n",
    "!ls mit_ner/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13CzQt8BD2SY"
   },
   "source": [
    "Perfect. Now we see all our files. Let's poke at one of them before we start parsing our dataset.\n",
    "\n",
    "The file is a 2-column tab separated file. The first column is the label and the second column is the token.\n",
    "\n",
    "If you need a refresher of IOB label encoding, an O label on a token denotes that the token is not part of a named entity. A \"B\" label indicates that the token is the start of a named entity, with the description following the \"B-\" describing the type of entity. An \"I\" label on a token indicates that the token is part of a named entity but not the first token in the entity, with the description following the \"I-\" describing the type of entity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FjGTQR_GGUSl"
   },
   "source": [
    "TRAIN_FILE = \"mit_ner/engtrain.bio\"\n",
    "TEST_FILE = \"mit_ner/engtest.bio\"\n",
    "\n",
    "# The file is a 2-column tab separated file. \n",
    "# The first column is the label and the second column is the token.\n",
    "# Empty rows refer to a sentence break\n",
    "with open(TRAIN_FILE, 'r', newline='\\n') as file:\n",
    "  reader = csv.reader(file, delimiter = '\\t')\n",
    "  i = 0\n",
    "  while(i < 15):\n",
    "    print(next(reader))\n",
    "    i += 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2TGoMlWSP33"
   },
   "source": [
    "Empty rows refer to a sentence break. Now we should try and parse the dataset file and create a label encoder that converts text labels to integer ids or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nxVX02RBk7_G"
   },
   "source": [
    "def parse_dataset(file_path, label_encoder):\n",
    "  \"\"\"\n",
    "  Function to parse the csv into training or test dataset\n",
    "\n",
    "  Input: Tuple[label,token] or Empty\n",
    "  Output: List of tuples of label and token. Each list is one sentence.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(file_path, 'r') as file:\n",
    "    reader = csv.reader(file, delimiter = '\\t')\n",
    "    sentence=[]\n",
    "    for row in reader:\n",
    "      if len(row) == 0:\n",
    "        data.append(sentence)\n",
    "        sentence=[]\n",
    "      else:\n",
    "        sentence.append((label_encoder([row[0]])[0], row[1]))\n",
    "  return data\n",
    "\n",
    "# Custom encoder that creates the encoding in reverse order so to assign 'O' -> 0\n",
    "# The default scikit LabelEncoder doesn't allow for that \n",
    "# Ref: https://stackoverflow.com/questions/51308994/python-sklearn-determine-the-encoding-order-of-labelencoder\n",
    "class CustomLabelEncoder():\n",
    "  def __init__(self):\n",
    "    self.classes_ = None\n",
    "    self.transform_map = None\n",
    "    self.inverse_map = None\n",
    "  \n",
    "  def fit(self, y):\n",
    "    self.transform_map = {}\n",
    "    self.inverse_map = {}\n",
    "    self.classes_ = np.unique(y)[::-1] # [\"I-PLOT\", ... unique labels]\n",
    "    for i, c in enumerate(self.classes_):\n",
    "      self.transform_map[c] = i\n",
    "      self.inverse_map[i] = c # \"c\" is a label, e.g. \"I-PLOT\"\n",
    "    return self\n",
    "\n",
    "  def transform(self, y):\n",
    "    return [self.transform_map.get(x) for x in y]\n",
    "\n",
    "  def inverse_transform(self, y):\n",
    "    return [self.inverse_map.get(x) for x in y]\n",
    "\n",
    "# A lable encoder converts the text labels into integer ids\n",
    "def get_label_encoder():\n",
    "  \"\"\"Get all the labels in a dataset and return two maps that convert labels -> id or vice versa.\n",
    "  \"\"\"\n",
    "  # We pass an identity encoder since we still need the raw labels to train the label encoder\n",
    "  raw_data = parse_dataset(TRAIN_FILE, lambda x: x)\n",
    "  le = CustomLabelEncoder()\n",
    "  le.fit([x[0] for sentence in raw_data for x in sentence])\n",
    "  return le\n",
    "\n",
    "# Global variables used throughout the notebook\n",
    "label_encoder = get_label_encoder()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tjJ1kSxMU2LJ"
   },
   "source": [
    "label_encoder.classes_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDl8D4XhTp0q"
   },
   "source": [
    "Creating the global training, validation, and test datasets from the data files."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2NPRscoJTqec"
   },
   "source": [
    "training_data = parse_dataset(TRAIN_FILE, label_encoder.transform)\n",
    "test_data = parse_dataset(TEST_FILE, label_encoder.transform)\n",
    "\n",
    "print('# of training examples: {rows}'.format(rows=len(training_data)))\n",
    "print('# of test examples: {rows}'.format(rows=len(test_data)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-Uqb3NpWNoD"
   },
   "source": [
    "[Dataset and Data loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html): Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "[LightingDataModule](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html#datamodules): A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "\n",
    "1. Download / tokenize / process.\n",
    "2. Clean and (maybe) save to disk.\n",
    "3. Load inside Dataset.\n",
    "4. Apply transforms (rotate, tokenize, etcâ€¦).\n",
    "5. Wrap inside a DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PUdUpboqyyUn"
   },
   "source": [
    "class NERDataset(Dataset):\n",
    "  \"\"\"Creates an pytorch dataset to consume our pre-loaded csv data\n",
    "\n",
    "  Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html \n",
    "  \"\"\"\n",
    "  def __init__(self, data, vectorizer):\n",
    "    self.dataset = data\n",
    "    # Vectorizer needs to implement a vectorize function that returns vector and tokens\n",
    "    # ðŸŒŸðŸŒŸðŸŒŸ Pay extra attention here since you'll have to work on this in the models ðŸŒŸðŸŒŸðŸŒŸ\n",
    "    self.vectorizer = vectorizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    words_and_label = self.dataset[idx]\n",
    "    # Split the input into two different lists per sentence\n",
    "    labels, words = list(zip(*words_and_label))\n",
    "    token_vectors, tokens, aligned_labels = self.vectorizer.vectorize(words, labels)\n",
    "    return {\n",
    "        \"vectors\": np.array(token_vectors),\n",
    "        \"tokens\": tokens, # for debugging only\n",
    "        \"labels\": aligned_labels\n",
    "      }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3ejJP8oKRqN"
   },
   "source": [
    "We're going to digress for 3 seconds to learn a bit more about the sequence padding function. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "knHK1jfZKb-d"
   },
   "source": [
    "a = torch.ones(22)\n",
    "b = torch.ones(15)\n",
    "c = torch.ones(25)\n",
    "print(\"One dimensional Sequence\", pad_sequence([a, b, c]).size())\n",
    "a = torch.ones(22, 300)\n",
    "b = torch.ones(25, 300)\n",
    "c = torch.ones(15, 300)\n",
    "print(\"Two dimensional Sequence\", pad_sequence([a, b, c]).size())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJf7siuWKsi9"
   },
   "source": [
    "It'll make all the sequences of the max length in the list. Now back to writing the data module wrapper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XoSZNDnwy66i"
   },
   "source": [
    "class NERDataModule(pl.LightningDataModule):\n",
    "  \"\"\"LightningDataModule: Wrapper class for the dataset to be used in training, test, validation\n",
    "  \"\"\"\n",
    "  def __init__(self, vectorizer, params):\n",
    "    super().__init__()\n",
    "    self.params = params\n",
    "    self.ner_test = NERDataset(test_data, vectorizer)\n",
    "\n",
    "    # The dataset doesn't have an explicit validation data, so we split the training data into an 80-20 split\n",
    "    ner_train_val_combined = NERDataset(training_data, vectorizer)\n",
    "\n",
    "    # Compute the 80-20 split\n",
    "    train_sample_size = int(len(ner_train_val_combined) * 0.8)\n",
    "    validation_sample_size = len(ner_train_val_combined)- train_sample_size\n",
    "\n",
    "    # Create the actual training and validation datasets\n",
    "    self.ner_train, self.ner_val = random_split(ner_train_val_combined, [train_sample_size, validation_sample_size])\n",
    "  \n",
    "  # Function to convert the input raw data from the dataset into model input. \n",
    "  # ðŸŒŸðŸŒŸðŸŒŸ Pay extra attention here since you'll have to work on this in the models ðŸŒŸðŸŒŸðŸŒŸ\n",
    "  def collate_fn(self, batch):\n",
    "    # Separate out the vectors and labels from the batch\n",
    "    word_vector = [torch.Tensor(item[\"vectors\"]) for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Now pad each vector sequence to the same size\n",
    "    padded_word_vector = pad_sequence(word_vector)\n",
    "\n",
    "    # Max sequence length is the max size of a single sentence in this batch\n",
    "    max_seq_len = padded_word_vector.shape[0]\n",
    "\n",
    "    # Pad the label vector as well\n",
    "    padded_label_vector = []\n",
    "    for label in labels:\n",
    "      padded_label = np.concatenate((label,np.array([0] * (max_seq_len - len(label)))))\n",
    "      padded_label_vector.append(padded_label)\n",
    "    padded_label_vector = torch.LongTensor(np.array(padded_label_vector))\n",
    "\n",
    "    # Mask to make sure when computing the loss we're not accounting for the padding\n",
    "    padded_mask = torch.logical_not(torch.all(torch.isclose(padded_word_vector, torch.tensor(0.0)), dim=-1)).int()\n",
    "\n",
    "    return {\"vectors\": padded_word_vector, \"label_ids\": padded_label_vector, \"mask\": torch.transpose(padded_mask, 0, 1)}\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # Training dataloader .. will reset itself each epoch\n",
    "    return DataLoader(self.ner_train, batch_size=self.params.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    # Validation dataloader .. will reset itself each epoch\n",
    "    return DataLoader(self.ner_val, batch_size=self.params.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    # Test dataloader .. will reset itself each epoch\n",
    "    return DataLoader(self.ner_test, batch_size=self.params.batch_size, collate_fn=self.collate_fn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W1XWmeAvqZ9"
   },
   "source": [
    "### Classfier and Trainer (Common to all solutions)\n",
    "\n",
    "Now that we have created the DataLoader and Datasets we'll use in the entire project, it is time to write the training and testing loops. \n",
    "\n",
    "[LightingModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html): organizes your PyTorch code into 5 sections\n",
    "\n",
    "1. Computations (init).\n",
    "2. Train loop (training_step)\n",
    "3. Validation loop (validation_step)\n",
    "4. Test loop (test_step)\n",
    "5. Optimizers (configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3whW_bSZL3rk"
   },
   "source": [
    "# ðŸŒŸðŸŒŸðŸŒŸ Pay extra attention here since you'll have to work on this in the models ðŸŒŸðŸŒŸðŸŒŸ\n",
    "class NamedEntityRecognizer(pl.LightningModule):\n",
    "  def __init__(self, model, params):\n",
    "      super().__init__()\n",
    "      self.model = model\n",
    "      self.params = params\n",
    "      self.validation_accuracy = torchmetrics.Accuracy(multiclass=True, mdmc_reduce=\"global\")\n",
    "      self.test_accuracy = torchmetrics.Accuracy(multiclass=True, mdmc_reduce=\"global\")\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"label_ids\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    y_hat = self(x).permute(0, 2, 1)\n",
    "    loss = F.cross_entropy(y_hat, y, reduction='none')\n",
    "    active_loss = torch.mean(torch.multiply(loss, mask))\n",
    "    self.log_dict(\n",
    "        {'train_loss': active_loss}, \n",
    "        batch_size=self.params.batch_size, \n",
    "        prog_bar=True\n",
    "        )\n",
    "    return active_loss\n",
    "  \n",
    "  def validation_step(self, batch, batch_nb):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"label_ids\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    y_hat = self(x)\n",
    "    val_loss = F.cross_entropy(y_hat.permute(0, 2, 1), y, reduction='none')\n",
    "    active_loss = torch.mean(torch.multiply(val_loss, mask))\n",
    "    predictions = torch.argmax(y_hat, dim=2)\n",
    "\n",
    "    # We apply with the mask here to zero out any predictons that are just made \n",
    "    # on the padding from being added to the accuracy calculation\n",
    "    self.validation_accuracy(predictions*mask, y)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'val_loss': active_loss,\n",
    "          'val_accuracy': self.validation_accuracy,\n",
    "        },\n",
    "        batch_size=self.params.batch_size, \n",
    "        prog_bar=True\n",
    "      )\n",
    "    return active_loss\n",
    "\n",
    "  def test_step(self, batch, batch_nb):\n",
    "    x = batch[\"vectors\"]\n",
    "    y = batch[\"label_ids\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    y_hat = self(x)\n",
    "    test_loss = F.cross_entropy(y_hat.permute(0, 2, 1), y, reduction='none')\n",
    "    active_loss = torch.mean(torch.multiply(test_loss, mask))\n",
    "    predictions = torch.argmax(y_hat, dim=2)\n",
    "    # We apply with the mask here to zero out any predictons that are just made \n",
    "    # on the padding from being added to the accuracy calculation\n",
    "    self.test_accuracy(predictions*mask, y)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'test_loss': active_loss,\n",
    "          'test_accuracy': self.test_accuracy\n",
    "        }, \n",
    "        batch_size=self.params.batch_size,\n",
    "        prog_bar=True\n",
    "      )\n",
    "    return active_loss\n",
    "  \n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[\"vectors\"])\n",
    "    predictions = torch.argmax(y_hat, dim=2)\n",
    "    return {'logits':y_hat, 'predictions': predictions, 'label_ids': batch[\"label_ids\"], 'mask': batch[\"mask\"]}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.params.learning_rate)\n",
    "    return optimizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyqbOirR5A6o"
   },
   "source": [
    "We'll need a Vectorizer to convert words into word vectors. This week we'll just freeze a simple Spacy-based vectorizer and use that for all the models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QBXTEjzH9uHA"
   },
   "source": [
    "class SpacyVectorizer:\n",
    "  def vectorize(self, words, labels):\n",
    "    \"\"\"\n",
    "    Given a sentence, tokenize it and returns a pre-trained word vector for each token.\n",
    "    NOTE: note that the vectorizer needs to be compatible with the tokenizer.\n",
    "    \"\"\"\n",
    "    assert len(words) == len(labels)\n",
    "    sentence_vector = []\n",
    "    sentence_labels = []\n",
    "    sentence_tokens = []\n",
    "    for i, word in enumerate(words):\n",
    "      label = labels[i]\n",
    "\n",
    "      # Tokenize the words using spacy\n",
    "      spacy_doc = loaded_spacy_model.make_doc(word)\n",
    "      word_vector = [token.vector for token in spacy_doc]\n",
    "      word_tokens = [token.text for token in spacy_doc]\n",
    "      sentence_vector += word_vector\n",
    "      sentence_tokens += word_tokens\n",
    "\n",
    "      # Make sure the length of the labels is same as length of vectors\n",
    "      # In out current dataset we get pre tokenized words so this shouldn't actually happen\n",
    "      word_labels = [label] * len(word_vector)\n",
    "      sentence_labels += word_labels\n",
    "    return sentence_vector, sentence_tokens, sentence_labels\n",
    "\n",
    "vectorizer = SpacyVectorizer()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8FakcAGVTsQ"
   },
   "source": [
    "Once we have a Lightning and LightingDataModule, a [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) automates everything else. It provides functions for training (fit), testing, and inference. We ended up writing a helper function that takes the model, vectorizer, and hyper parameters to be able to easily compare our different models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qW2nbktK9su3"
   },
   "source": [
    "def trainer(model, params, skip_training=False):\n",
    "  # Create a pytorch trainer\n",
    "  trainer = pl.Trainer(max_epochs=params.max_epochs, check_val_every_n_epoch=1)\n",
    "\n",
    "  # Initialize our data loader with the passed vectorizer\n",
    "  data_module = NERDataModule(vectorizer, params)\n",
    "\n",
    "  # Train and validate the model\n",
    "  if not skip_training:\n",
    "    trainer.fit(model,\n",
    "                data_module.train_dataloader(), \n",
    "                val_dataloaders=data_module.val_dataloader())\n",
    "\n",
    "  # Test the model\n",
    "  trainer.test(model, data_module.test_dataloader())\n",
    "\n",
    "  # Predict on the same test set to show some output\n",
    "  output = trainer.predict(model, data_module.test_dataloader())\n",
    "\n",
    "  for i in range(2):\n",
    "    print(\"-----------\")\n",
    "    print(\"Label Ids: \", [label_encoder.inverse_transform([int(x)])[0] for x in output[0]['label_ids'][i]])\n",
    "    print(\"Predictions: \", [label_encoder.inverse_transform([int(x)])[0] for x in output[0]['predictions'][i]])\n",
    "    print(\"Mask: \", output[0]['mask'][i])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtZuKF-Y3kkB"
   },
   "source": [
    "# Models\n",
    "\n",
    "You may be wondering, \"Are we building models yet?\" And, the answer is YES! Finally the time has come to build our baseline model, and then we'll work towards improving it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6sWp1NFyZWZ"
   },
   "source": [
    "### Assignment Part 1: Model 0: Make them all ZERO  -- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~74 - 79%</font>\n",
    "\n",
    "Build a model that doesn't do any predictions and just returns 0 ('O') for all label ids to be predicted.\n",
    "\n",
    "This is a good baseline for our model since most of our predictions are 0 anyway so we should get some decent accuracy with just this. Building a model only makes sense if we can at least do better than this :) "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ONxGcjI3l5Ze"
   },
   "source": [
    "class HParamsZERO:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "\n",
    "class NERModelZero(torch.nn.Module):\n",
    "  def __init__(self, num_tags):\n",
    "    super().__init__()\n",
    "    self.num_tags = num_tags\n",
    "    \n",
    "  def forward(self, batch):\n",
    "    \"\"\"Batch is of shape (max_seq_len, batch_size, word_vector_dim)\n",
    "    Returns: Output probabities of shape (batch_size, max_seq_len, num_tags)\n",
    "    \"\"\"\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    return ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizer(NERModelZero(HParamsZERO.num_tags), \n",
    "                                HParamsZERO),\n",
    "    params=HParamsZERO,\n",
    "    skip_training=True)  # NOTE: We skip training here since there is no variable to learn in this model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4UMdWBr3sqo"
   },
   "source": [
    "### Assignment Part 2: Model 1: Recurrent Neural Networks  -- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~86 - 92%</font>\n",
    "\n",
    "Let's build our first [Simple RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) based model. \n",
    "\n",
    "Here we've two key pieces:\n",
    "\n",
    "1. *HParams*: a class that contains all the hyper parameters we reference in the notebook.\n",
    "1. *NERModelRNN*: Simple RNN model and it's forward pass implementation. The model should contain one RNN layer, followed by one dropout layer and then one linear layer to compute the output.\n",
    "\n",
    "It will take several minutes to train the model, so don't be alarmed if you don't get the result right away. When the cell finishes running, under the `DATALOADER:0 TEST RESULTS` section."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yAlU02AXFIkS"
   },
   "source": [
    "class HParamsRNN:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "  hidden_dim: int = 128 # Added\n",
    "  dropout: float = 0.2 # Added\n",
    "  word_vec_dim: int = 300 # Added\n",
    "\n",
    "class NERModelRNN(torch.nn.Module):\n",
    "  def __init__(self, num_tags):\n",
    "    super().__init__()\n",
    "    # ðŸŒŸðŸŒŸðŸŒŸ WOOHOO!! We're using RNNs in the model\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    self.rnn = ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    self.dropout = torch.nn.Dropout(HParamsRNN.dropout)\n",
    "    self.tags = torch.nn.Linear(HParamsRNN.hidden_dim, num_tags)\n",
    "    \n",
    "  def forward(self, batch):\n",
    "    \"\"\"Batch is of shape (max_seq_len, batch_size, word_vector_dim)\n",
    "    \n",
    "    Observe that each batch contains sentence encodings of equal length (the length of the longest sentence).\n",
    "    This is because sentences are variable in length; however, processing sentences\n",
    "    in batches via (variants of) RNNs needs to be of equal length. We have already added padding, \n",
    "    to make each batch the length of the longest sentence in the batch (see NERDataModule.collate_fn()).\n",
    "\n",
    "    Given the batch, do the following:\n",
    "    - Implement a hidden state for an RNN.\n",
    "    - Use the hidden state with the batch for the RNN.\n",
    "    - Put the RNN through the other layers (dropout, tags).\n",
    "    - Put the outcome in the appropriate format.\n",
    "\n",
    "    Returns: Output probabities of shape (batch_size, max_seq_len, num_tags)\n",
    "    \"\"\"\n",
    "    # Propagate input through RNN layer\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    # Propagate input through RNN layer\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    # Propogate the rnn output through dropout and linear layers\n",
    "    final_output = self.tags(self.dropout(rnn_output))\n",
    "    return torch.transpose(final_output, 0, 1)\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizer(NERModelRNN(HParamsRNN.num_tags), \n",
    "                                HParamsRNN),\n",
    "    params=HParamsRNN)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8NZivXzeqhq"
   },
   "source": [
    "ðŸŽ‰ðŸŽ‰ðŸŽ‰ WE HAVE OUR NER Model that uses RNNs ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "Now might be a good time to 1) celebrate!, and 2) re-read the code to internalize it."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment Part 3: Model 2: Bidirectional Recurrent Neural Networks -- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~86 - 91%</font>\n",
    "\n",
    "Now that you've got a [simple RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) based model working, we like to add one small addition. This addition, as you might've read from the PyTorch documentation, you can apply to any RNN based model. \n",
    "\n",
    "We do this by adding a few parameters which are used to implement bidirectionality for a RNN model."
   ],
   "metadata": {
    "id": "Hjae2kTUuNC-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HParamsBiRNN:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "  hidden_dim: int = 128\n",
    "  dropout: float = 0.2\n",
    "  word_vec_dim: int = 300\n",
    "  num_layers: int = 2 # Added\n",
    "  bidirectional: bool = True # Added\n",
    "\n",
    "class NERModelRNN(torch.nn.Module):\n",
    "  def __init__(self, num_tags):\n",
    "    super().__init__()\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    self.dropout = torch.nn.Dropout(HParamsBiRNN.dropout)\n",
    "    \n",
    "  def forward(self, batch):\n",
    "    \"\"\"Batch is of shape (max_seq_len, batch_size, word_vector_dim)\n",
    "\n",
    "    Returns: Output probabities of shape (batch_size, max_seq_len, num_tags)\n",
    "    \"\"\"\n",
    "    # Propagate input through RNN layer\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "    # Propogate the rnn output through dropout and linear layers\n",
    "    final_output = self.tags(self.dropout(rnn_output))\n",
    "    return torch.transpose(final_output, 0, 1)\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizer(NERModelRNN(HParamsBiRNN.num_tags), \n",
    "                                HParamsBiRNN),\n",
    "    params=HParamsBiRNN)"
   ],
   "metadata": {
    "id": "Dw0inN_SuKFx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice! Do we see any performance improvements? What will the change do to the runtime? \n",
    "\n",
    "We'd like to encourage you to add this option to later models as well and compare."
   ],
   "metadata": {
    "id": "Cldvw9Glw5o9"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gYWFusQ39qF"
   },
   "source": [
    "### Assignment Part 4: Model 3: LSTMs ---- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~89 - 96%</font>\n",
    "\n",
    "We'll be re-using the simple RNN model from Model-1 but changing the layers from RNN to LSTMs. \n",
    "\n",
    "1. Change the RNN layer to [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) Layer\n",
    "2. Keep the dropout and linear layers in the model\n",
    "3. Complete the forward pass implementation\n",
    "\n",
    "Does this model perform better than our baseline?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YOIulG2Tgpb1"
   },
   "source": [
    "class HParamsLSTM:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "  ### TO BE IMPLEMENTED --- Some parameters are missing here so add them ##\n",
    "  ...\n",
    "  ### TO BE IMPLEMENTED ###\n",
    "\n",
    "class NERModelLSTM(torch.nn.Module):\n",
    "  def __init__(self, num_tags):\n",
    "    \"\"\"\n",
    "    Three layers: LSTM, dropout and linear chained one after the other.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "  def forward(self, batch):\n",
    "    \"\"\"Batch is of shape (max_seq_len, batch_size, word_vector_dim)\n",
    "    Returns: Output probabities of shape (batch_size, max_seq_len, num_tags)\n",
    "    \"\"\"\n",
    "    # Propagate input through LSTM\n",
    "\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "    # Propogate the LSTM output through dropout and linear layers\n",
    "    \n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    # when you only consider the nr of parameters.\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    return torch.transpose(final_output, 0, 1)\n",
    "\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizer(NERModelLSTM(len(label_encoder.classes_)), \n",
    "                                HParamsLSTM),\n",
    "    params=HParamsLSTM)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvJfO82Fwhup"
   },
   "source": [
    "Play around with some predictions? On what examples does the LSTM do much better than the RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCdfbXZIkyKi"
   },
   "source": [
    "### Assignment Part 5: Model 4: Stacked LSTMs ---- TO BE COMPLETED\n",
    "##### <font color='red'>Both expected accuracy: ~88 - 96%</font>\n",
    "\n",
    "So far we have created single layer LSTM and RNNs. Now change the code to create a new stacked LSTM with 2 and 4. How do they perform? \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sn4vO-rVkxB4"
   },
   "source": [
    "# 2 Stacked LSTM\n",
    "class HParamsLSTMS2:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  max_seq_len: int = 20\n",
    "  hidden_dim: int = 128\n",
    "  word_vec_dim: int = 300\n",
    "  num_layers: int = 1 # Adapt\n",
    "  dropout: float = 0.2\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizer(NERModelLSTM(HParamsLSTMS2.num_tags), \n",
    "                                HParamsLSTMS2),\n",
    "    params=HParamsLSTMS2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5LTURuiqlR6D"
   },
   "source": [
    "# 4 Stacked LSTM\n",
    "class HParamsLSTMS4:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  max_seq_len: int = 20\n",
    "  hidden_dim: int = 128\n",
    "  word_vec_dim: int = 300\n",
    "  num_layers: int = 1 # Adapt\n",
    "  dropout: float = 0.2\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizer(NERModelLSTM(HParamsLSTMS4.num_tags), \n",
    "                                HParamsLSTMS4),\n",
    "    params=HParamsLSTMS4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WgEFNfyxJ7-"
   },
   "source": [
    "### Assignment Part 6: Is Accuracy a good metric? \n",
    "\n",
    "We've been using Accuracy as a metric for the last two weeks. Is this a good metric for this problem? Debate and write a short answer below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCuNHTiLxlhN"
   },
   "source": [
    "---\n",
    "\n",
    "**YOUR ANSWER TO BE FILLED HERE**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qVUi5ip4EHU"
   },
   "source": [
    "### Assignment Part 7: Implement a new LightningModule. --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~76 - 80%, expected F1: ~2 - 5%</font>\n",
    "\n",
    "We're going to compute more metrics on our models. Let's create a new training module which adds **F1 Score** to it.\n",
    "\n",
    "**NamedEntityRecognizerWithF1**: Implement some functions we've seen multiple times now\n",
    "\n",
    "1. Initialize the F1 metric correctly with *mdmc_average:global* and *average:macro*.\n",
    "1. `validation_step`: Write the function that computes validation_loss. It should be really close to what you've seen before.\n",
    "1. `test_step`: Write the function that computes test_loss. It should be really close to what you've seen before.\n",
    "1. `predict_step`: Implement a function that does inference here. Will anything need to change here?\n",
    "1. `configure_optimizers`: Configure adam optimizer to run at the learning rate from the hyper parameters. Will anything need to change here?\n",
    "\n",
    "\n",
    "Good explanation: [of micro vs macro averages for multiclass classification](https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/24051#24051)\n",
    "\n",
    "Another reading for metrics on multi-class classification is [here](https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X_Y1_k4DitV5"
   },
   "source": [
    "from torchmetrics import F1Score\n",
    "\n",
    "\n",
    "class HParamsF1:\n",
    "  batch_size: int = 32\n",
    "  learning_rate: float = 0.02\n",
    "  max_epochs: int = 4\n",
    "  max_seq_len: int = 20\n",
    "  hidden_dim: int = 128\n",
    "  word_vec_dim: int = 300\n",
    "  num_layers: int = 1\n",
    "  dropout: float = 0.2\n",
    "  num_tags: int = len(label_encoder.classes_)\n",
    "\n",
    "class NamedEntityRecognizerWithF1(pl.LightningModule):\n",
    "  def __init__(self, model, params):\n",
    "      super().__init__()\n",
    "      self.model = model\n",
    "      self.params = params\n",
    "      self.validation_accuracy = torchmetrics.Accuracy(multiclass=True, mdmc_reduce=\"global\")\n",
    "      self.test_accuracy = torchmetrics.Accuracy(multiclass=True, mdmc_reduce=\"global\")\n",
    "      ### ADD YOUR NEW METRICS ### \n",
    "      ### TO BE IMPLEMENTED ###\n",
    "      ...\n",
    "      ### TO BE IMPLEMENTED ###  \n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    return active_loss\n",
    "  \n",
    "  def validation_step(self, batch, batch_nb):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    return active_loss\n",
    "\n",
    "  def test_step(self, batch, batch_nb):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    return active_loss\n",
    "  \n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    ...\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "# Train, Validate and Test the model\n",
    "trainer(\n",
    "    model=NamedEntityRecognizerWithF1(NERModelZero(HParamsF1.num_tags), HParamsF1),\n",
    "    params=HParamsF1,\n",
    "    skip_training=True)  # NOTE: We skip training here since there is no variable to learn in this model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHJa8YEltfSA"
   },
   "source": [
    "#### Let's re-run our RNN and LSTM models once with the new metrics\n",
    "##### <font color='red'>Expected accuracy: ~82 - 90%, expected F1: ~32 - 36%</font>\n",
    "##### <font color='red'>Expected accuracy: ~88 - 96%, expected F1: ~65 - 70%</font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajyckZDGtyE2"
   },
   "source": [
    "trainer(\n",
    "    model=NamedEntityRecognizerWithF1(NERModelRNN(HParamsRNN.num_tags), HParamsRNN),\n",
    "    params=HParamsRNN)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-19yhuXrtd82"
   },
   "source": [
    "trainer(\n",
    "    model=NamedEntityRecognizerWithF1(NERModelLSTM(HParamsLSTM.num_tags), HParamsLSTM),\n",
    "    params=HParamsLSTM)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZvWAI7soxvP"
   },
   "source": [
    "ðŸŽ‰ CONGRATS on finishing the assignment!!! Now is a good time to pause and reflect how much progress we've made in understanding NER, using RNNs, and LSTMs. But hey, don't stop here! There is a lot to do or play with in the next section.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM0pHSbtGHuj"
   },
   "source": [
    "# Extensions\n",
    "\n",
    "Now that you've worked through the project. There is a lot more for us to try.\n",
    "\n",
    "- See if you can use a LSTM to improve the model you shipped in Week 1.\n",
    "- Which model performed the best? Why do you think that was?\n",
    "- Why and how many stacks can we keep adding here ?\n",
    "- Experiment with the hyper parameters to see how much can you improve each of the models\n",
    "- Use a conditional random field (CRF) along with LSTM. [Tutorial](https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463)\n",
    "- Visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html) of N*N of actual class vs predicted class (N = number of classes)\n",
    "- Try out other [NER datasets](https://github.com/juand-r/entity-recognition-datasets) on our models."
   ]
  }
 ]
}
